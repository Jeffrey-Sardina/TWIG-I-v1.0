['run_exp.py', '0', 'UMLS', '10', '5e-4', 'zscore', '1024', '5', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 6 loss: 0.13012035191059113
Epoch 2 -- batch 0 / 6 loss: 0.13035733997821808
Epoch 3 -- batch 0 / 6 loss: 0.1217404156923294
Epoch 4 -- batch 0 / 6 loss: 0.12055609375238419
Epoch 5 -- batch 0 / 6 loss: 0.12390600889921188
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 6 loss: 0.115391805768013
Epoch 7 -- batch 0 / 6 loss: 0.11889294534921646
Epoch 8 -- batch 0 / 6 loss: 0.11323517560958862
Epoch 9 -- batch 0 / 6 loss: 0.11465203762054443
Epoch 10 -- batch 0 / 6 loss: 0.11588642746210098
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 1
ranks: tensor([178., 230.,  87., 146.,  44.,   1.,   1.,   1.,  11.,  30.,  45.,  28.,
         12., 108., 122.,  75., 172.,  26.,  65., 228., 254., 165., 109.,  26.,
         82., 158.,  71.,  12.,  66., 249., 239., 249., 250.,  25., 236., 114.,
        250., 246., 250., 223., 224., 231., 233., 230., 232.,  99., 240., 242.,
        255.,  17.,  22.,   8., 223.,  14., 231., 242., 237.,   4.,  15.,  12.,
        243., 172., 232., 237., 236., 222., 225., 212., 219., 124.,  52., 197.,
        204.,   6.,   1.,   2.,   1., 123., 203., 195.,   1.,  15.,  38., 136.,
        215., 138.,  13., 199., 134., 130., 240., 264., 257.,  24., 165., 104.,
        241., 242., 228., 116.,  63., 196., 203., 143., 199., 242., 250., 253.,
        234., 257., 237.,  27., 153.,  25.,  28.,  19.,  36., 250., 220., 252.,
        141., 242.,  29., 132., 246.,  12.,  16.,  86.,  32.,  23., 206., 225.,
         13.,  19., 152., 102., 127.,  15., 203., 258., 249., 126., 193., 196.,
          1.,   1.,   1.,  74., 199., 209., 212., 200., 207.,  18.,  66.,  13.,
          1.,   1.,   1., 124.,   6., 223., 146., 131., 226., 201., 100.,   1.,
         22.,  80., 231., 229.,  21., 133., 137.,  17.,  29., 157.,   7.,  11.,
         10.,  45.,   5.,  77.,   9., 144.,  16.,  24.,   9.,  15., 253., 186.,
        228., 138.,  87., 187.,   1.,   1.,  12., 161.,  83., 169.,  88., 135.,
        128., 195.,  65., 230.,   6.,   8., 184., 198., 182., 170., 172., 172.,
          1.,  58.,  74.,  15., 139., 230.,  22.,  60., 125., 241., 240., 244.,
        242., 258.,  77.,   9.,  95., 248.,  51., 188., 246., 175., 247., 242.,
        234., 243.,  19.,  18.,   5.,  25., 239., 189., 237., 246., 187., 248.,
        171., 209., 210.,  89.,  60., 139., 100.,  58., 240., 231., 243.,  58.,
         43.,  28., 103., 114., 243., 120., 221., 223., 160.,   1., 221., 143.,
        212.,  54.,  19.,  36., 178., 202.,  46.,   1.,  18.,  78.,  16., 102.,
        236.,   1., 192., 135.,  49., 111.,  43., 253., 243., 206.,   6.,   4.,
          7., 191.,  14.,   1., 233., 195., 177., 161., 167.,  19., 230., 230.,
        218., 236., 241., 240., 102., 188., 236., 253., 102., 252., 207., 135.,
         16.,  14.,  65., 185., 212., 193., 207., 203.,  16., 142.,   1.,  87.,
        192., 228.,   4., 223., 236., 236.,  83., 208., 110.,  54., 155., 236.,
        232., 211., 211., 143., 198.,  91.,  31.,  52., 134., 107.,  47., 213.,
        239., 237., 237., 241., 230., 240., 235., 229., 233., 124., 228.,   6.,
        186., 201., 127.,   8., 241., 137.,  99.,  93., 228.,   1., 118., 248.,
        249.,  46., 260., 179.,  75.,  14., 255., 147., 137., 185., 153., 181.,
         13., 228.,  81., 111., 118., 125., 100., 183.,  20.,  20., 157., 175.,
          1., 140., 137.,  22.,  20., 157., 244., 122.,  97., 130.,   1., 191.,
        167.,   7.,   8., 152., 207., 230.,   5.,  12., 253., 256., 189., 244.,
        243., 237., 251.,   3., 256.,  14., 200.,  99., 135., 255., 143., 200.,
        211., 210.,  35.,  29., 191.,  13., 190., 172., 223., 220., 189.,  42.,
          7.,   1.,  66.,   6., 102.,  30.,   1.,   1., 124., 190.,  30.,  78.,
        113., 130.,  13., 130., 150.,   1.,  18.,   2.,   8.,  17.,  69., 209.,
        220., 214., 214., 217., 133.,  23., 215.,  31., 207., 190.,   9.,   4.,
        180.,  15., 200., 208., 134., 223., 237., 123., 243.,   1.,   2., 153.,
        136., 222., 222., 226.,   7., 225., 218., 104., 212.,  17., 109., 103.,
        132., 147., 198.,  48.,  45., 195.,  78.,  10., 203., 207.,  21.,  59.,
        102.,  75.,   1.,   1.,   3., 211., 134.,  58.,  84.,  24.,  73.,   8.,
        229., 230., 205., 181.,  11., 213.,  12., 192.,  39., 237., 230., 240.,
          7., 191., 116.,  74.,  60., 210., 246., 246.,  62., 210., 162., 158.,
        233., 246., 235.,   5.,  23., 133., 235., 190., 136., 125., 148.,  10.,
        130., 217., 124., 110., 169.,   1., 115.,  67., 204., 202., 186., 182.,
          1.,   1., 205., 203., 167.,  13.,  69., 160.,   3.,   5.,  50.,   3.,
         76.,  22.,  17., 106.,   6.,  73., 117., 136., 176., 132.,   1., 191.,
          1., 155.,  99.,  25., 127., 135.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1., 146.,   1.,  56.,   1.,  57.,  35.,   1.,   1.,   1.,   1.,
        102.,  35.,   1.,   1.,  12., 123.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 124.15950775146484
mrr: 0.12051330506801605
h1: 0.09509202092885971
h3: 0.10582821816205978
h5: 0.11963190138339996
h10: 0.15950919687747955
==================================

Done Testing!
done with training and eval
Experiments took 37 seconds
