['run_exp.py', '0', 'UMLS', '10', '5e-4', 'zscore', '256', '5', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 21 loss: 0.13397477567195892
Epoch 2 -- batch 0 / 21 loss: 0.11393618583679199
Epoch 3 -- batch 0 / 21 loss: 0.11917977780103683
Epoch 4 -- batch 0 / 21 loss: 0.09811289608478546
Epoch 5 -- batch 0 / 21 loss: 0.07993226498365402
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 21 loss: 0.09210436791181564
Epoch 7 -- batch 0 / 21 loss: 0.07061824947595596
Epoch 8 -- batch 0 / 21 loss: 0.0728687047958374
Epoch 9 -- batch 0 / 21 loss: 0.07822238653898239
Epoch 10 -- batch 0 / 21 loss: 0.07107338309288025
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 3
ranks: tensor([107., 162.,  13.,  52.,   8.,   1.,   1.,   1.,   8.,   8.,  12.,   6.,
          9.,  10.,   9.,   9.,  95.,   9.,   4., 197., 255.,  50.,  26.,   1.,
         91., 152.,  53.,  10.,  48., 190., 248., 112., 250.,  81., 246., 147.,
        191., 226., 251.,  33., 141., 148., 108., 136.,  93.,  76., 243., 242.,
        255.,  14.,  16.,   8., 132.,   7., 111., 180., 243.,   4.,   9.,   7.,
         41.,   7., 199., 102., 202., 222.,  11., 167., 197.,  17.,   7.,  93.,
        112.,   1.,   1.,   4.,   1.,  32.,  69.,  92.,   1.,   3.,   8.,  51.,
         75.,  77.,   3., 156., 133., 100., 112., 264., 246.,  26.,  41.,  32.,
        200., 139., 177.,  38.,   1.,  15.,  33.,  38., 152., 250., 250.,  21.,
        243., 257.,  78.,   1.,  14.,   1.,   1.,   1.,   1., 255.,  13., 237.,
         14.,   1.,   1.,  26.,   1.,   1.,   1.,  19.,  13.,   9.,   1.,  26.,
          1.,   1.,  84.,   1.,  28.,   1.,  73., 254., 254.,  79., 202., 193.,
          1.,   1.,   1.,  94., 176., 202., 209.,  67., 106.,   1.,  21.,   4.,
          1.,   2.,   1.,  82.,   5., 219.,  76.,   7., 187., 116.,  21.,   1.,
          1.,   5.,  59., 104.,   2.,  27.,  47.,  15.,   2.,  34.,   1.,   1.,
          1.,  42.,   1.,  10.,   1.,  10.,  10.,  10.,   1.,   7., 149.,  84.,
        116.,  10.,   9.,  68.,   1.,   1.,   1., 100.,   1.,  21.,   3.,  65.,
         14., 157.,   8.,   9.,   4.,   8., 178., 198., 144., 159., 198., 138.,
          1.,  58.,  38.,  13.,  59., 184.,  13.,  14.,  98., 122., 128., 191.,
        208.,  73.,  51.,   8.,  33., 247.,  18., 167., 165.,   4.,   1.,   1.,
          1.,  93.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.,  29.,  27.,  39.,  95.,  41., 146.,  53.,   7.,
         18.,  60., 164., 142., 133., 235., 225.,   9.,   1.,   1.,  30.,  36.,
         42.,  19.,   4.,   2.,  23.,   7.,  26.,   1.,   1.,  11.,   6.,   1.,
         35.,   3.,   5.,  38.,   7.,   7., 147.,  92., 148., 170.,   1.,   1.,
          1., 137.,  72.,   1., 129., 241., 242., 243., 142.,   7.,  97.,  28.,
         27., 206., 148., 145.,   1.,  52.,  53.,   7.,   1.,  69.,   1., 135.,
          4.,   1.,   1.,  48.,   5., 140., 205., 194.,   3.,  46.,  98., 144.,
        136., 193.,  99., 162., 102., 225.,  93., 127.,  44.,  91., 110., 236.,
        233.,  71., 154., 141., 132.,  16., 113., 109., 234., 188., 134., 224.,
        136.,  63., 120.,  24.,   8.,  20.,   8.,   9.,  17.,   1.,  15.,   1.,
          1.,   1.,   1.,   1., 142., 222.,  92.,  73., 204.,   1., 105., 159.,
        251.,  92., 259.,  74., 132., 121.,  98., 106., 131.,  25.,  10.,  11.,
         51., 146.,   7.,   9.,  43.,   1.,   1., 130.,  82., 109., 124.,   7.,
          1.,  10.,   7.,   2.,   9.,   6.,  61., 172.,  15., 134.,   1.,  17.,
         32.,   1.,  31.,   1.,   1.,  18.,  59.,  17., 146., 133.,   3., 113.,
        116., 245.,   7.,   1.,   5.,   1.,  18.,   8.,  43., 207.,  93., 186.,
        218., 204.,   4.,   4.,  88.,  77., 193.,  87.,   3.,   6.,   7.,   1.,
          1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   4.,  67.,   6.,   1.,
         39.,  84.,   2., 138.,  95.,  30.,   1.,   1.,   1.,   5.,   4.,  47.,
         92., 228., 226., 226.,  10.,  64.,  75.,   1.,  81., 105.,  12.,   1.,
        108.,  25.,  96., 121.,  14.,  86.,   2.,   1.,   1.,   1.,   1.,   1.,
          1., 190., 225., 227.,   3.,   1.,  65.,  13.,   2.,   8.,  76.,   1.,
        138.,  86., 177.,  15.,  63.,  39.,  36.,   2.,  77.,  83.,   5.,   4.,
          1.,   1.,   1.,   1.,   1., 151., 109.,  70.,  41.,  64., 133., 125.,
        233., 234., 194.,  93.,  11., 145.,   2., 185., 212., 102., 236., 242.,
          1., 125., 135.,  34., 116., 231., 246., 134., 158., 246., 180., 105.,
        244.,   4., 242.,  17.,  89.,  64., 237.,  79.,  70.,  53., 115.,   5.,
        205., 227., 134., 134., 169.,   1., 135., 112., 212., 178., 199.,  23.,
         23.,   6.,  47.,  14.,  61.,   2.,   1.,   2.,  96., 117.,  93.,  38.,
        116.,  89., 101., 177.,  18.,  17.,  70.,   8.,  19.,  30.,  45., 242.,
        254.,  46.,   1.,   1.,  49.,  94., 248., 253.,  18.,  86., 230., 248.,
          1.,  38., 167.,   3.,  14.,   1.,   6.,   3.,   1.,   1.,   1.,   1.,
          4.,  11.,   1.,   1.,   1.,  31.,  54.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 72.37269592285156
mrr: 0.2618851959705353
h1: 0.21625766158103943
h3: 0.25
h5: 0.2852760851383209
h10: 0.3757668733596802
==================================

Done Testing!
done with training and eval
Experiments took 35 seconds
