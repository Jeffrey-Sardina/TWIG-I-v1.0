['run_exp.py', '0', 'UMLS', '10', '5e-6', 'zscore', '1024', '5', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 6 loss: 0.13155724108219147
Epoch 2 -- batch 0 / 6 loss: 0.13376843929290771
Epoch 3 -- batch 0 / 6 loss: 0.12855958938598633
Epoch 4 -- batch 0 / 6 loss: 0.12818117439746857
Epoch 5 -- batch 0 / 6 loss: 0.13591399788856506
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 6 loss: 0.13009048998355865
Epoch 7 -- batch 0 / 6 loss: 0.12888936698436737
Epoch 8 -- batch 0 / 6 loss: 0.1303819864988327
Epoch 9 -- batch 0 / 6 loss: 0.12679032981395721
Epoch 10 -- batch 0 / 6 loss: 0.1322847604751587
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 1
ranks: tensor([184., 230., 170., 202., 189.,  25.,  27.,  47., 105., 175., 172., 138.,
        135., 202., 192., 187., 230.,  75., 207., 230., 254., 127.,  99.,  67.,
         58., 164.,  83.,  90., 127., 247., 242., 249., 250.,  19., 225., 129.,
        250., 248., 245., 240., 229., 225., 232., 231., 231.,  34., 243., 242.,
        255.,  66.,  91.,  16., 233.,  17., 236., 242., 236.,   9.,  18.,  15.,
        243., 233., 208., 220., 177., 222., 225., 217., 220., 159., 130., 146.,
        169.,  28.,   1.,   7.,   1., 130., 203., 193.,   6.,  34.,  78., 145.,
        213.,  71.,  25., 207., 134., 128., 242., 262., 259.,  24., 164.,  68.,
        240., 242., 216.,  87., 145., 235., 128., 116., 217., 218., 250., 253.,
         78., 257., 230.,  28.,  58.,  14.,  11.,   7.,  25., 250., 234., 252.,
         61., 242.,  75., 213., 248.,  28.,  29., 150., 125.,  74., 222., 209.,
        202.,  42., 157., 205., 128., 225., 246., 258., 240., 192., 146., 120.,
          1.,   1.,   1.,  23., 198., 212., 211., 202., 207.,  64., 172.,  80.,
          7.,   1.,   1., 126., 109., 223., 119.,  90., 212., 184., 193.,   9.,
        101., 101., 230., 229.,  55., 143.,  44.,  17.,  79., 177.,  17.,  11.,
         10.,  45.,  13., 103.,  11., 220., 145., 158.,  13.,  40., 268., 217.,
        254., 245., 197., 230.,   1.,  18., 220., 159., 233., 187., 143., 135.,
        131., 224., 143., 230.,  24.,  13., 225., 198., 189., 180.,  93., 188.,
          1., 133.,  13.,  16., 147., 230., 166., 182., 205., 230., 229., 242.,
        235., 258.,  93.,   3.,  16., 248., 219., 236., 248., 222., 246., 234.,
        243., 243., 118., 106.,  14., 217., 242., 231., 239., 246., 236., 248.,
        171., 203., 144.,  93.,  31.,  97.,  36.,   6., 238., 166., 243.,  93.,
         13.,  10.,  34., 100., 243.,  71., 221., 223., 220.,  11., 223., 197.,
        215.,  49.,  89.,  32., 191., 201.,  49.,   6., 137., 107.,  28., 131.,
        236.,   1., 165., 135.,  57., 205., 214., 256., 243., 146.,   8.,   9.,
          7., 207.,  23.,   1., 231., 199., 169., 138., 181., 217., 237., 201.,
        197., 220., 231., 213., 116., 204., 224., 253., 160., 252., 202., 135.,
         33.,  27., 103., 196., 212., 183., 207., 204., 169., 217., 147.,   1.,
        164., 226.,  29., 223., 232., 234.,  68., 179., 135., 147., 211., 236.,
        234., 194., 155.,  89., 191.,  82.,  26.,  28.,  21.,  25.,  38., 156.,
        231., 237., 240., 239., 220., 234., 226., 215., 236., 131., 220.,   1.,
         33., 200., 141.,   8., 241.,  34., 103.,  90., 226.,   1., 134., 246.,
        222.,  79., 260., 202., 142.,  39., 255.,  55., 122., 163., 164., 138.,
         25., 123., 130., 127., 129., 198., 180., 213.,  27.,  26., 193., 223.,
        162., 220., 220., 201.,  78., 231., 247., 202.,  60., 130.,   1., 212.,
        232.,  26.,  18., 205., 211., 132.,   1.,  75., 250., 256., 169., 228.,
        232., 245., 250.,   6., 254.,  25., 235., 132., 135., 255.,  28., 189.,
        156., 191.,  34.,  27., 191.,   4., 192., 193., 223., 221., 192., 115.,
          7.,   1., 122.,   6., 161.,  47., 110., 116., 219., 213., 116., 141.,
        119., 128., 185., 196., 201., 106.,  35.,  22.,   9.,  23.,  90., 197.,
        216., 182., 175., 191., 166.,  18., 210.,  27., 209., 208.,  28.,  13.,
        204.,  24., 206., 134., 148., 220., 243., 227., 243.,   5.,  17., 218.,
        223., 221., 208., 226.,  23., 222., 220., 197., 216., 110.,  82., 133.,
         69.,  88., 134.,  48.,  21., 182.,  64., 153., 176., 207.,  36., 152.,
         80., 122.,   2.,   6.,  12., 241., 221., 215., 125., 205.,  93.,  33.,
        227., 227., 202., 205.,  16., 215.,  41.,  33.,   3., 237., 222., 234.,
         11., 232., 232., 160., 202., 198., 246., 246.,  45., 221., 144., 135.,
        229., 246., 236.,  35., 110., 190., 234., 192.,  58.,  75., 139.,  15.,
          7., 151., 121.,  72., 169.,   1., 109., 176., 189., 169., 193., 183.,
          3.,   1., 196., 206., 190.,  14.,  94., 190.,  37.,  37.,  48.,   6.,
         89.,  48.,  47., 145.,  13.,  75., 147., 128., 206., 131.,   1., 110.,
          1., 139., 105.,  37., 125., 135.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1., 159.,   1., 143.,   1.,  69.,  52.,   1.,   1.,   1.,   1.,
        143., 124.,   1.,   1.,  16., 123.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 137.3542938232422
mrr: 0.09102896600961685
h1: 0.07208588719367981
h3: 0.07822085916996002
h5: 0.08128834515810013
h10: 0.11349692940711975
==================================

Done Testing!
done with training and eval
Experiments took 42 seconds
