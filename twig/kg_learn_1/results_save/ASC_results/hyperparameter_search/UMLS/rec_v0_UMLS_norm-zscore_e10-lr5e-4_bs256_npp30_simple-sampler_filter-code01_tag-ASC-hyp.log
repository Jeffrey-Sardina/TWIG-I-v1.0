['run_exp.py', '0', 'UMLS', '10', '5e-4', 'zscore', '256', '30', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 21 loss: 0.13467255234718323
Epoch 2 -- batch 0 / 21 loss: 0.11169537156820297
Epoch 3 -- batch 0 / 21 loss: 0.11700446903705597
Epoch 4 -- batch 0 / 21 loss: 0.09228795021772385
Epoch 5 -- batch 0 / 21 loss: 0.07880217581987381
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 21 loss: 0.0878925621509552
Epoch 7 -- batch 0 / 21 loss: 0.07018739730119705
Epoch 8 -- batch 0 / 21 loss: 0.0737101286649704
Epoch 9 -- batch 0 / 21 loss: 0.08046773076057434
Epoch 10 -- batch 0 / 21 loss: 0.06873758882284164
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 3
ranks: tensor([116., 161.,  13.,  38.,   8.,   1.,   1.,   1.,   7.,   9.,  10.,   7.,
          8.,  13.,   5.,  11.,  94.,  11.,   5., 208., 255.,  55.,  25.,   1.,
         72., 154.,  45.,   2.,  45., 186., 247., 115., 250., 103., 239., 145.,
        192., 208., 251.,  25., 141., 134.,  91., 121.,  97.,  77., 241., 236.,
        255.,  16.,  16.,   4., 138.,   7., 102., 172., 243.,   5.,   8.,   8.,
         38.,   7., 197.,  88., 190., 222.,  15., 169., 198.,   9.,   9.,  88.,
         92.,   3.,   1.,   4.,   1.,  25.,  66.,  82.,   1.,   3.,  10.,  46.,
         80.,  86.,   5., 145., 134., 101., 125., 264., 249.,  27.,  40.,  41.,
        198., 129., 176.,  34.,   1.,  17.,  33.,  38., 198., 250., 250.,  20.,
        241., 257.,  63.,   1.,  11.,   1.,   1.,   1.,   1., 251.,  15., 227.,
          9.,   1.,   1.,  13.,   1.,   1.,   1.,  19.,   7.,   8.,   1.,  23.,
          1.,   1.,  85.,   1.,  20.,   1.,  82., 253., 255.,  52., 201., 185.,
          1.,   1.,   1.,  89., 168., 190., 207.,  69., 115.,   1.,  37.,   4.,
          1.,   2.,   1.,  85.,   5., 219.,  76.,  10., 177., 116.,  15.,   1.,
          1.,   5.,  54., 110.,   1.,  25.,  43.,  18.,   1.,  27.,   1.,   1.,
          1.,  51.,   1.,  11.,   1.,   3.,   6.,  11.,   1.,  10., 152.,  76.,
         94.,  11.,  10.,  69.,   1.,   1.,   1.,  92.,   1.,  16.,   6.,  55.,
         11., 159.,   8.,  11.,   5.,  10., 191., 198., 149., 171., 198., 142.,
          4.,  57.,  36.,  13.,  45., 191.,   9.,  12.,  72., 110., 119., 172.,
        193.,  75.,  49.,  11.,  44., 249.,  19., 163., 165.,  12.,   1.,   1.,
          1.,  75.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.,  30.,  24.,  45., 109.,  40., 143.,  56.,   8.,
         21.,  81., 166., 148., 130., 237., 225.,   4.,   1.,   1.,  30.,  35.,
         43.,  22.,   5.,   2.,  25.,   7.,  27.,   1.,   1.,  11.,   4.,   2.,
         32.,   4.,   3.,  55.,   8.,   5., 158.,  70., 154., 135.,   1.,   1.,
          1., 136.,  81.,   1., 125., 241., 241., 242., 138.,   5., 123.,  29.,
         27., 207., 133., 140.,   1.,  55.,  34.,  11.,   1.,  53.,   1., 135.,
          1.,   1.,   1.,  50.,   5., 130., 203., 195.,   7.,  44., 101., 135.,
        128., 193.,  91., 150.,  86., 219.,  89., 126.,  36., 100.,  91., 236.,
        228.,  68., 151., 147., 110.,  13., 109., 121., 225., 183., 134., 222.,
        136.,  61., 122.,  21.,   9.,  18.,   8.,  10.,  19.,   1.,  23.,   1.,
          1.,   1.,   1.,   1., 153., 216.,  93.,  65., 190.,   1.,  91., 158.,
        242.,  91., 254.,  85., 123., 116.,  80., 119., 111.,  25.,  16.,  25.,
         78., 115.,   5.,  10.,  34.,   1.,   1., 133.,  75.,  99., 119.,   8.,
          1.,   8.,   7.,   1.,   4.,   3.,  46., 117.,  15., 132.,   1.,  18.,
         27.,   1.,  32.,   1.,   1.,  15.,  66.,  18., 157., 144.,   1.,  63.,
         53., 245.,   8.,   1.,   5.,   1.,  17.,   6.,  34., 227.,  85., 175.,
        213., 202.,   6.,   5.,  91.,  77., 175.,  78.,   3.,   2.,   4.,   1.,
          1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   3.,  68.,   6.,   1.,
         35.,  81.,   1., 126.,  86.,  31.,   1.,   1.,   1.,   5.,   5.,  26.,
         93., 228., 226., 226.,  12.,  39.,  78.,   1.,  73., 105.,   7.,   1.,
        110.,  25.,  91., 120.,  14.,  90.,   1.,   1.,   1.,   1.,   1.,   1.,
          1., 171., 213., 227.,   4.,   2.,  63.,  12.,   1.,   7.,  75.,   1.,
        173.,  76., 180.,  12.,  64.,  42.,  37.,   1.,  65.,  44.,   6.,   3.,
          4.,   1.,   1.,   1.,   1., 142., 108.,  62.,  34.,  68., 143., 121.,
        234., 234., 182.,  95.,  15., 128.,   3., 195., 208.,  94., 236., 242.,
          1., 136., 131.,  28.,  97., 234., 246., 120., 161., 246., 181., 100.,
        243.,   7., 242.,  22.,  96.,  66., 236.,  88.,  79.,  70., 115.,   6.,
        205., 227., 135., 134., 169.,   1., 135., 108., 211., 176., 196.,  21.,
         22.,   6.,  48.,  14.,  60.,   2.,   1.,   2.,  98., 114.,  99.,  38.,
        112.,  75., 102., 152.,  13.,  11.,  82.,  10.,  44.,  37.,  82., 242.,
        253.,  47.,   1.,   1.,  54., 101., 243., 250.,  30.,  91., 224., 248.,
          1.,  23., 167.,   3.,  15.,   1.,   6.,   4.,   1.,   1.,   1.,   1.,
          1.,  10.,   1.,   1.,   2.,  39.,  51.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 71.02914428710938
mrr: 0.2659946084022522
h1: 0.22239263355731964
h3: 0.25153374671936035
h5: 0.29447853565216064
h10: 0.3680981695652008
==================================

Done Testing!
done with training and eval
Experiments took 39 seconds
