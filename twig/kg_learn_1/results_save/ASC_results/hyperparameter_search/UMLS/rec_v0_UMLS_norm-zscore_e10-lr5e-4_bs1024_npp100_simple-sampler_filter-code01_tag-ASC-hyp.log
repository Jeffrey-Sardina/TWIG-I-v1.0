['run_exp.py', '0', 'UMLS', '10', '5e-4', 'zscore', '1024', '100', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 6 loss: 0.1325201839208603
Epoch 2 -- batch 0 / 6 loss: 0.12930722534656525
Epoch 3 -- batch 0 / 6 loss: 0.124295674264431
Epoch 4 -- batch 0 / 6 loss: 0.12360581755638123
Epoch 5 -- batch 0 / 6 loss: 0.12954919040203094
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 6 loss: 0.12189371138811111
Epoch 7 -- batch 0 / 6 loss: 0.11243844777345657
Epoch 8 -- batch 0 / 6 loss: 0.1149088442325592
Epoch 9 -- batch 0 / 6 loss: 0.11004119366407394
Epoch 10 -- batch 0 / 6 loss: 0.11461137235164642
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 1
ranks: tensor([185., 230., 106., 170.,  11.,   1.,   1.,   1.,  10.,  10.,  22.,   7.,
         10.,  92., 106.,  69., 166.,  32.,  50., 236., 255., 234.,  41.,  11.,
         56., 165.,  62.,  12.,  61., 250., 247., 249., 250.,  67., 247.,  99.,
        248., 250., 251., 139., 207., 228., 231., 238., 230.,  79., 243., 243.,
        255.,  15.,  23.,  10., 224.,  18., 222., 243., 243.,  14.,  18.,  15.,
        238.,  18., 241., 236., 240., 222., 182., 220., 220.,  97.,  39., 208.,
        211.,   6.,   1.,   4.,   1.,  65., 203., 196.,   1.,  16.,  53.,  89.,
        218., 139.,  23., 215., 135., 134., 239., 264., 265.,  39., 168., 122.,
        249., 249., 242., 194.,  84., 195., 214., 179., 215., 250., 250., 253.,
        249., 257., 252.,  30., 190.,  82.,  96.,  19., 122., 255., 214., 257.,
        157.,  92.,  35., 144., 227.,   7.,  10.,  88.,  32.,  11., 188., 225.,
         10.,  20., 150.,  80., 118.,   8., 197., 258., 258., 104., 177., 168.,
          1.,   1.,   1.,  40., 197., 213., 212., 199., 207.,   6., 147.,  49.,
          1.,   2.,   2., 179.,  16., 223., 178., 129., 236., 209.,  30.,   1.,
         18.,  41., 231., 229.,  28., 139., 152.,  19.,  28., 168.,   1.,  14.,
          2.,  58.,   1.,  74.,   2., 121.,  16.,  16.,   8.,  13., 258., 196.,
        226., 134.,  94., 188.,   1.,   1.,  10., 158.,  68., 151.,  91., 135.,
        129., 207.,  72., 188.,   8.,   9., 197., 198., 154., 147., 171., 150.,
          1.,  33.,  25.,  15., 139., 230.,  35.,  64., 106., 248., 252., 250.,
        250., 258., 110.,   6.,  88., 249.,  24., 208., 250., 161., 249., 249.,
        238., 243.,  19.,  20.,   7.,  19., 240., 179., 239., 246., 144., 251.,
        158., 224., 219.,  99.,  72., 134., 128.,  67., 171., 222., 243.,  53.,
         49.,  30., 123., 142., 243., 140., 225., 223., 180.,   1., 222., 164.,
        219.,  72.,  21.,  20., 164., 170.,  42.,   1.,  17.,  69.,  17.,  95.,
        236.,   1., 180., 135.,  78., 111.,  30., 249., 243., 205.,   1.,   1.,
          1., 161.,  20.,   1., 191., 217., 195., 192., 130.,  15., 246., 243.,
        240., 246., 251., 250., 106., 199., 230., 253.,  62., 252., 205., 135.,
         20.,  12.,  84., 202., 212., 197., 207., 207.,  65., 117.,   2., 100.,
        194., 226.,   6., 223., 240., 236.,  73., 189., 103.,  67., 188., 236.,
        241., 215., 230., 188., 222., 133.,  86., 146., 172., 138.,  56., 236.,
        246., 245., 245., 243., 244., 245., 244., 241., 242., 132., 238.,  15.,
        217., 229., 155.,  81., 241., 164., 144., 100., 235.,   1., 114., 260.,
        259.,  58., 260., 191., 135.,  22., 255., 171., 163., 219., 175., 212.,
         16., 239.,  68.,  85.,  86., 126.,  87., 194.,  12.,  22., 168., 142.,
          1., 121., 118.,  16.,  26., 132., 244., 131., 119., 135.,   1., 174.,
        170.,   5.,   6., 136., 208., 244.,   7.,  25., 256., 256., 192., 252.,
        253., 240., 256.,   2., 256.,   4.,  35.,  66., 135., 255., 104., 197.,
        195., 213.,  15.,  16., 191.,  14., 196., 171., 223., 223., 195.,   8.,
          7.,   1.,  61.,   1.,  99.,   7.,   1.,   1.,  76., 185.,  27.,  52.,
         75., 131.,   2., 134., 170.,   1.,  22.,   6.,   7.,  19.,  53., 206.,
        218., 216., 224., 224.,  44.,   8., 218.,   6., 176., 180.,   8.,   5.,
        169.,  10., 191., 204., 112., 176.,  95.,  43., 243.,   1.,   2.,  56.,
        120., 227., 227., 227.,  14., 189., 221., 105., 178.,  60., 144., 111.,
        137., 154., 213.,  41.,  98., 180.,  77.,  11., 211., 212.,  84.,  67.,
        115.,  72.,   1.,   1.,   5., 218.,  75.,  19.,  80.,  25.,  82.,   7.,
        234., 234., 212., 156.,   9., 199.,  54., 166.,  32., 237., 236., 242.,
          9., 173., 104.,  69.,  57., 197., 246., 246.,  27., 223., 190., 153.,
        239., 245., 242.,   9.,  39., 154., 242., 193., 180., 153., 156.,  13.,
        147., 233., 133., 121., 169.,   1., 130.,  42., 212., 208., 183., 162.,
          1.,   1., 194., 203., 163.,  15.,  62., 164.,   7.,   8.,  52.,   5.,
         72.,  22.,   7.,  82.,  44.,  54., 138., 131., 185., 124.,   1., 145.,
          1., 158.,  58.,  13.,  97., 135.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1., 136.,   1.,  47.,   1.,  58.,  35.,   1.,   1.,   1.,   1.,
         87.,  47.,   1.,   1.,  20.,  95.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 123.78681182861328
mrr: 0.12661710381507874
h1: 0.09969325363636017
h3: 0.11196319013834
h5: 0.12116564065217972
h10: 0.17638036608695984
==================================

Done Testing!
done with training and eval
Experiments took 42 seconds
