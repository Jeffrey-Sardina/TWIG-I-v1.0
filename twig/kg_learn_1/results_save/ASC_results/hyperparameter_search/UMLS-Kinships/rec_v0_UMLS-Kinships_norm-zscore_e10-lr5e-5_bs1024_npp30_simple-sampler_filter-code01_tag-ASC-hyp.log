['run_exp.py', '0', 'UMLS-Kinships', '10', '5e-5', 'zscore', '1024', '30', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
Kinships
X_p: torch.Size([8544, 37])
X_p: torch.Size([1074, 37])
X_p: torch.Size([1068, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 15 loss: 0.13161048293113708
Epoch 2 -- batch 0 / 15 loss: 0.1322898119688034
Epoch 3 -- batch 0 / 15 loss: 0.12720246613025665
Epoch 4 -- batch 0 / 15 loss: 0.12265016883611679
Epoch 5 -- batch 0 / 15 loss: 0.1271687150001526
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 15 loss: 0.13609574735164642
Epoch 7 -- batch 0 / 15 loss: 0.12494653463363647
Epoch 8 -- batch 0 / 15 loss: 0.12824779748916626
Epoch 9 -- batch 0 / 15 loss: 0.133087620139122
Epoch 10 -- batch 0 / 15 loss: 0.12605808675289154
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 1
ranks: tensor([193., 230., 180., 205., 180.,   2.,   8.,  22., 143., 119., 137.,  83.,
         95., 188., 190., 182., 225.,  79., 202., 238., 255., 146.,  47.,  56.,
         43., 161.,  88., 111., 113., 235., 236., 249., 250.,  18., 217., 139.,
        243., 244., 244., 241., 213., 218., 229., 226., 225.,  46., 241., 241.,
        255.,  95.,  43.,  18., 237.,  19., 227., 242., 238.,  17.,  15.,  18.,
        242., 231., 207., 232., 222., 222., 224., 213., 220., 142., 123., 201.,
        205.,  27.,   1.,  55.,   1., 112., 203., 209.,   3.,  40.,  85., 136.,
        218., 103.,  40., 204., 133., 131., 234., 263., 260.,  24., 166.,  75.,
        234., 234., 213.,  87., 132., 228., 134.,  95., 222., 197., 250., 253.,
         81., 257., 234.,  56., 123.,  25.,  22.,  24.,  31., 247., 208., 249.,
        186., 231.,  62., 194., 243.,  25.,  25., 135.,  68.,  50., 189., 197.,
        161.,  33., 158., 199., 124., 191., 248., 258., 242., 163., 159., 163.,
          1.,   1.,   1.,  25., 190., 209., 209., 196., 207., 136., 155.,  33.,
          7.,   1.,   1., 152.,  46., 223., 123., 185., 197., 181., 144.,   6.,
        103.,  78., 231., 229., 106., 120., 133.,  29.,  84., 185.,  16.,  13.,
          2.,  64.,  12., 101.,  13., 215., 109., 112.,  12.,  39., 268., 208.,
        254., 230., 184., 215.,   1.,   1., 205., 164., 218., 183., 128., 135.,
        128., 220., 151., 230.,  46.,  26., 230., 198., 187., 181.,  73., 192.,
          1.,  74.,  16.,  14., 144., 230., 163., 208., 186., 228., 234., 233.,
        230., 258., 134.,   8.,  25., 247., 153., 247., 249., 228., 241., 233.,
        245., 243.,  89.,  75.,  11., 171., 234., 227., 231., 246., 229., 243.,
        167., 209., 166., 107.,  81., 141.,  97.,  32., 230., 155., 243.,  94.,
         38.,  23.,  28.,  97., 243.,  62., 218., 223., 221.,  10., 223., 209.,
        217.,  69., 186.,  39., 203., 194.,  74.,   2., 108.,  85.,  15., 122.,
        236.,   1., 178., 135.,  78., 198., 230., 256., 243., 202.,   7.,   1.,
          1., 211., 146.,   1., 196., 183., 167., 149., 176., 186., 245., 204.,
        204., 233., 235., 207., 124., 205., 204., 253., 136., 252., 201., 135.,
         34.,  35., 121., 201., 212., 189., 207., 200., 172., 216., 133.,   3.,
        177., 225.,  51., 223., 225., 236.,  51., 178., 166., 112., 204., 236.,
        226., 179., 193., 118., 190.,  84.,  34.,  27.,  56.,  43.,  37., 161.,
        230., 232., 240., 232., 199., 209., 201., 179., 227., 134., 200.,   6.,
         83., 194., 173.,  12., 241.,  61.,  94.,  98., 206.,   6., 133., 247.,
        222.,  79., 260., 209., 204.,  40., 255., 117., 113., 163., 193., 185.,
         61.,  99., 110., 119., 126., 190., 140., 218., 133., 127., 172., 193.,
         77., 200., 204., 176.,  77., 202., 248., 201., 149., 132.,   1., 203.,
        203.,  21.,  18., 191., 211., 167.,   6.,  76., 247., 254., 187., 217.,
        218., 245., 250.,   2., 253.,  18., 227., 126., 135., 255.,  21.,  76.,
        150., 167.,  16.,  15., 191.,   1., 186., 182., 219., 221., 191.,  94.,
          8.,   1., 112.,   7., 128.,  39.,  59.,  51., 211., 208.,  91., 125.,
        125., 129., 169., 199., 207.,  56.,  31.,  34.,   9.,  31., 110., 178.,
        201., 189., 185., 182., 166.,  15., 200.,  14., 198., 190.,  23.,  14.,
        188.,  22., 204., 166., 128., 210., 236., 198., 243.,   1.,   9., 212.,
        216., 212., 197., 224.,  24., 216., 212., 173., 194., 100.,  78., 193.,
        132.,  89., 116.,  45.,  15., 161.,  57., 169., 166., 208.,  98., 160.,
        112., 133.,   1.,   7.,   9., 219., 216., 195., 120., 205., 126.,  13.,
        228., 231., 202., 201.,  15., 195.,  78.,  27.,   7., 237., 224., 231.,
         10., 203., 223., 165., 187., 191., 246., 246.,  37., 205., 205.,  73.,
        224., 246., 237., 110.,  68., 198., 237., 194.,  75., 193., 116.,  55.,
         11., 163., 123.,  66., 169.,   1., 107., 165., 184., 167., 186., 169.,
          2.,   1., 177., 196., 183.,  24.,  89., 199.,  13.,  16.,  61.,   3.,
         93.,  29.,  15., 163.,  44.,  55., 150., 153., 189., 133.,   1., 153.,
          1., 135.,  83.,  36., 111., 135.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1., 154.,   1., 123.,   1.,  72.,  56.,   1.,   1.,   1.,   1.,
        147.,  95.,   1.,   1.,  76.,  99.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 136.82208251953125
mrr: 0.09490867704153061
h1: 0.07515337318181992
h3: 0.08742330968379974
h5: 0.08742330968379974
h10: 0.11349692940711975
Testing (cite this): dataloader for dataset Kinships
ranks: tensor([193., 230., 180.,  ..., 180., 183.,  45.])
mr: 134.04273986816406
mrr: 0.12146738916635513
h1: 0.10174418240785599
h3: 0.11337209492921829
h5: 0.11918604373931885
h10: 0.14825581014156342
==================================

Done Testing!
done with training and eval
Experiments took 84 seconds
