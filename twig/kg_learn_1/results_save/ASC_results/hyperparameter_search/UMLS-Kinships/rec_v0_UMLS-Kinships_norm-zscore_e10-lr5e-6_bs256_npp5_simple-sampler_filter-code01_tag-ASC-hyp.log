['run_exp.py', '0', 'UMLS-Kinships', '10', '5e-6', 'zscore', '256', '5', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
Kinships
X_p: torch.Size([8544, 37])
X_p: torch.Size([1074, 37])
X_p: torch.Size([1068, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 55 loss: 0.13526450097560883
Epoch 2 -- batch 0 / 55 loss: 0.1366308480501175
Epoch 3 -- batch 0 / 55 loss: 0.13844354450702667
Epoch 4 -- batch 0 / 55 loss: 0.14598219096660614
Epoch 5 -- batch 0 / 55 loss: 0.13069476187229156
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 55 loss: 0.1373506486415863
Epoch 7 -- batch 0 / 55 loss: 0.1453084796667099
Epoch 8 -- batch 0 / 55 loss: 0.1448443979024887
Epoch 9 -- batch 0 / 55 loss: 0.13237309455871582
Epoch 10 -- batch 0 / 55 loss: 0.1451769322156906
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 3
ranks: tensor([184., 230., 175., 202., 188.,  21.,  32.,  56., 147., 169., 181., 132.,
        138., 199., 193., 189., 230.,  98., 209., 236., 254., 132.,  80.,  95.,
         50., 162.,  75., 108., 125., 238., 239., 249., 250.,  24., 221., 136.,
        249., 241., 243., 242., 222., 222., 227., 226., 225.,  33., 241., 241.,
        255.,  98.,  57.,  19., 231.,  17., 232., 241., 241.,  16.,  14.,  17.,
        243., 231., 213., 217., 167., 222., 221., 216., 220., 157., 133., 153.,
        168.,  19.,   2.,  32.,   1., 118., 203., 200.,   3.,  37.,  88., 147.,
        214.,  81.,  24., 198., 135., 128., 238., 262., 260.,  21., 165.,  58.,
        239., 240., 225.,  81., 142., 237., 126., 110., 225., 196., 250., 253.,
         63., 257., 231.,  32.,  92.,  19.,  20.,  17.,  27., 246., 226., 248.,
         77., 233.,  68., 199., 246.,  32.,  26., 141.,  92.,  48., 217., 200.,
        192.,  37., 161., 203., 132., 230., 243., 258., 234., 164., 147., 130.,
          1.,   1.,   1.,  24., 191., 203., 210., 201., 207., 151., 174., 116.,
          6.,   1.,   6., 149., 128., 223., 120., 167., 198., 179., 173.,   9.,
        100.,  86., 230., 229.,  55., 123.,  64.,  25.,  75., 182.,  20.,  14.,
         10.,  57.,  16., 110.,   9., 219., 161., 166.,   9.,  47., 268., 209.,
        254., 236., 188., 225.,   1.,  15., 210., 160., 229., 188., 130., 135.,
        132., 225., 172., 230.,  63.,  25., 230., 198., 185., 184.,  77., 188.,
          1., 135.,  12.,  17., 136., 230., 171., 209., 191., 229., 230., 239.,
        233.,  76., 124.,  15.,  16., 247., 195., 238., 165., 141.,   1.,   1.,
          1., 243.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1., 234., 160., 151.,  78., 242., 232., 243., 201.,
         56., 222., 170., 108., 243., 218., 216., 223., 213.,   2., 218., 117.,
        124.,  33.,  88.,  10., 103.,  49.,  95.,   8.,  87.,  91., 115.,  48.,
        234.,   6.,  25., 135., 219., 238., 241., 256., 237., 215.,  68.,  34.,
         32., 234., 216.,  18., 237., 232., 229., 230., 242., 241., 233., 199.,
         30., 229., 217., 202.,   9.,  56., 137., 253., 157., 252., 202., 135.,
         66.,   6.,  15., 123., 210., 169., 205., 115., 145., 207., 166.,  64.,
        180., 222., 119., 221., 204., 233.,  83., 138.,  62., 165., 161., 233.,
        228., 164., 196., 203., 208., 128., 220., 195., 214., 204., 203., 205.,
        207., 140., 240.,  44.,  40.,  31.,  32.,  29., 123.,   6.,  50.,   8.,
          9.,  24.,   8.,   1., 238., 242., 127., 120., 229.,   3., 129., 162.,
        198., 113., 258., 187., 148., 118., 255., 124., 196.,  77.,  33.,  50.,
        139., 188., 113., 107., 119.,   8.,   9., 202., 170., 176., 208., 220.,
        193., 202., 213., 180., 211., 214., 242., 227.,  20., 106.,   7., 246.,
        139.,  18., 137., 212., 162., 208., 117., 127., 256., 256., 200., 243.,
        246., 243., 256.,   1., 256.,   2., 172., 121., 135., 255., 128., 171.,
        128., 176.,   7.,   1., 191.,  19., 192., 189., 223., 223., 150.,   1.,
         26.,   6.,   9.,  12.,  44.,   7., 129., 140., 218., 166., 141., 158.,
         87.,  73.,  36., 177.,  93.,  53.,  12.,   1.,   1.,  13., 195., 197.,
        214., 182., 176., 183., 166.,  12., 209.,   1., 209., 206., 124.,  35.,
        200., 117., 207., 128., 124., 216.,  53.,   1.,   1.,   1.,   1.,   1.,
          1., 217., 201., 224.,  18., 215., 212.,  71.,  65.,  97.,  85., 154.,
        177., 131., 117.,  20., 124., 157., 184., 193., 187., 212., 159., 170.,
        122., 154.,   1.,   1.,   1., 233., 205., 182., 191., 191., 168., 105.,
        226., 230., 207., 225., 104., 209.,  30.,   6., 107., 237., 189., 236.,
         26., 226., 236., 197., 230., 230., 246., 246., 185., 241., 167.,  81.,
        225., 246., 237.,  48.,  96., 200., 233., 209.,  21.,  30.,  38., 129.,
        155.,  37., 133., 109., 169., 179., 108., 104., 186.,  80., 158., 180.,
         18.,   5., 177., 203., 184.,   6.,   1., 197., 115., 122., 106., 102.,
        115.,  93., 110., 226.,   1.,  16., 216.,  40.,  19., 221., 184., 239.,
        254., 146.,  13.,   9.,  75., 135., 248., 253.,   7.,  10., 251., 248.,
          1., 127., 141., 134., 202.,  15.,   7.,   2.,   1.,   1.,   1.,   1.,
         44., 124.,   1.,   1.,  81., 125., 152.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 134.46778869628906
mrr: 0.10929878801107407
h1: 0.0889570564031601
h3: 0.09815950691699982
h5: 0.09969325363636017
h10: 0.14263804256916046
Testing (cite this): dataloader for dataset Kinships
ranks: tensor([184., 230., 175.,  ..., 180., 183.,  94.])
mr: 138.0220947265625
mrr: 0.1047656312584877
h1: 0.0872092992067337
h3: 0.09534883499145508
h5: 0.10116279125213623
h10: 0.12965115904808044
==================================

Done Testing!
done with training and eval
Experiments took 74 seconds
