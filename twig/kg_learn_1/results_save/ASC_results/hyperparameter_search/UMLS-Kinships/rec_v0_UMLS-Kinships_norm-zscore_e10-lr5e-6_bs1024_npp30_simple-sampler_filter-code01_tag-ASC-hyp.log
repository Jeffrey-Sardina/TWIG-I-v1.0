['run_exp.py', '0', 'UMLS-Kinships', '10', '5e-6', 'zscore', '1024', '30', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
Kinships
X_p: torch.Size([8544, 37])
X_p: torch.Size([1074, 37])
X_p: torch.Size([1068, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 15 loss: 0.13168008625507355
Epoch 2 -- batch 0 / 15 loss: 0.13382795453071594
Epoch 3 -- batch 0 / 15 loss: 0.12778182327747345
Epoch 4 -- batch 0 / 15 loss: 0.12461817264556885
Epoch 5 -- batch 0 / 15 loss: 0.1274321973323822
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 15 loss: 0.13736781477928162
Epoch 7 -- batch 0 / 15 loss: 0.12828271090984344
Epoch 8 -- batch 0 / 15 loss: 0.13165698945522308
Epoch 9 -- batch 0 / 15 loss: 0.13520847260951996
Epoch 10 -- batch 0 / 15 loss: 0.127882719039917
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 1
ranks: tensor([184., 230., 172., 201., 190.,  25.,  35.,  65., 118., 171., 179., 138.,
        149., 196., 191., 186., 230.,  92., 205., 233., 254., 128.,  92.,  77.,
         55., 160.,  84., 109., 126., 244., 242., 249., 250.,  16., 224., 140.,
        250., 244., 245., 242., 227., 221., 229., 231., 220.,  35., 242., 241.,
        255.,  73.,  86.,  14., 234.,  17., 233., 242., 238.,  13.,  15.,  18.,
        243., 239., 205., 210., 153., 222., 228., 216., 220., 158., 137., 140.,
        168.,  24.,   2.,  11.,   1., 141., 203., 179.,   4.,  36.,  83., 153.,
        218.,  79.,  24., 208., 133., 131., 240., 261., 260.,  23., 166.,  63.,
        239., 239., 215.,  90., 143., 234., 130., 118., 218., 208., 250., 253.,
         80., 257., 228.,  28.,  68.,  16.,  15.,  12.,  24., 248., 234., 251.,
         50., 239.,  71., 211., 245.,  27.,  26., 149., 110.,  60., 213., 206.,
        201.,  42., 157., 205., 127., 222., 247., 258., 234., 188., 139., 116.,
          1.,   1.,   1.,  26., 193., 210., 209., 201., 207., 119., 179., 107.,
          9.,   1.,   7., 143., 131., 223., 119., 103., 204., 186., 180.,  10.,
        102.,  91., 230., 229.,  53., 138.,  45.,  21.,  51., 178.,  16.,  13.,
          8.,  67.,  15., 102.,  13., 219., 154., 163.,  12.,  44., 268., 214.,
        254., 240., 194., 225.,   1.,  21., 225., 164., 237., 191., 140., 135.,
        132., 224., 156., 230.,  50.,  31., 227., 198., 194., 181.,  81., 194.,
          1., 133.,  19.,  15., 145., 230., 156., 196., 200., 228., 230., 242.,
        234., 258., 102.,  11.,  15., 247., 206., 244., 250., 227., 243., 232.,
        246., 243., 104., 102.,  12., 208., 238., 232., 236., 246., 234., 248.,
        171., 207., 144., 102.,  45., 115.,  46.,  17., 234., 174., 243.,  99.,
         12.,  15.,  28., 103., 243.,  69., 217., 223., 221.,  10., 223., 199.,
        215.,  40., 119.,  34., 185., 202.,  65.,   2., 142., 103.,  29., 127.,
        236.,   2., 167., 135.,  71., 206., 226., 256., 243., 142.,   7.,   5.,
          8., 211.,  29.,   1., 228., 190., 163., 133., 179., 214., 238., 200.,
        204., 223., 232., 204., 124., 203., 224., 253., 152., 252., 201., 135.,
         36.,  31.,  93., 198., 212., 172., 207., 198., 172., 216., 159.,   1.,
        165., 224.,  47., 223., 227., 233.,  56., 178., 133., 130., 212., 236.,
        235., 182., 161., 109., 188.,  82.,  29.,  27.,  25.,  23.,  35., 160.,
        232., 237., 240., 237., 217., 226., 225., 213., 236., 126., 223.,   1.,
         40., 206., 148.,   9., 241.,  40., 101.,  97., 223.,   8., 133., 245.,
        216.,  83., 260., 209., 155.,  46., 255.,  53., 119., 165., 177., 146.,
         29., 109., 122., 121., 133., 194., 156., 215.,  25.,  29., 194., 221.,
        160., 223., 221., 211.,  79., 221., 248., 205.,  48., 132.,   1., 217.,
        224.,  25.,  17., 215., 211., 139.,   1.,  89., 247., 256., 151., 225.,
        229., 245., 250.,   3., 252.,  21., 235., 133., 135., 255.,  27., 183.,
        162., 189.,  19.,  20., 191.,   5., 190., 184., 221., 222., 191., 109.,
          7.,   1., 117.,   7., 143.,  43., 133., 113., 220., 220., 105., 138.,
        128., 130., 187., 189., 199., 113.,  37.,  33.,   9.,  21.,  98., 200.,
        214., 183., 170., 185., 172.,  16., 208.,  22., 211., 206.,  28.,  13.,
        206.,  23., 211., 135., 148., 220., 243., 222., 243.,   5.,  29., 225.,
        222., 216., 201., 225.,  25., 222., 218., 188., 208., 109.,  79., 141.,
         77., 100., 123.,  54.,  23., 185.,  63., 176., 160., 208.,  23., 155.,
         45., 120.,   1.,   7.,  11., 237., 221., 215., 135., 208., 108.,  39.,
        225., 229., 201., 201.,  18., 212.,  59.,  31.,   4., 237., 223., 229.,
         11., 227., 233., 158., 199., 193., 246., 246.,  43., 224., 155., 120.,
        228., 246., 235.,  70., 130., 197., 232., 193.,  60., 115., 135.,  28.,
         17., 152., 122.,  64., 169.,   1., 107., 177., 187., 165., 186., 182.,
          2.,   1., 188., 203., 183.,  18.,  90., 190.,  52.,  46.,  53.,   3.,
        102.,  58.,  55., 149.,  14.,  70., 158., 130., 201., 132.,   1., 110.,
          1., 138.,  97.,  45., 124., 135.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1., 162.,   1., 144.,   1.,  67.,  53.,   1.,   1.,   1.,   1.,
        147., 126.,   1.,   1.,  20., 109.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 137.8711700439453
mrr: 0.08745191991329193
h1: 0.06748466193675995
h3: 0.07668711990118027
h5: 0.08435583114624023
h10: 0.10429447889328003
Testing (cite this): dataloader for dataset Kinships
ranks: tensor([184., 230., 172.,  ..., 180., 183.,  93.])
mr: 140.35581970214844
mrr: 0.09051211923360825
h1: 0.07325581461191177
h3: 0.08139535039663315
h5: 0.08837209641933441
h10: 0.10872092843055725
==================================

Done Testing!
done with training and eval
Experiments took 75 seconds
