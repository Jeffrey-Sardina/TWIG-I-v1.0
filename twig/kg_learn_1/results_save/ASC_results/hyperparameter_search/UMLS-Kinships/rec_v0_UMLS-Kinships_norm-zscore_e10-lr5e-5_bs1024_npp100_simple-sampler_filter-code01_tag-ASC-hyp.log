['run_exp.py', '0', 'UMLS-Kinships', '10', '5e-5', 'zscore', '1024', '100', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
Kinships
X_p: torch.Size([8544, 37])
X_p: torch.Size([1074, 37])
X_p: torch.Size([1068, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 15 loss: 0.1325490027666092
Epoch 2 -- batch 0 / 15 loss: 0.12998907268047333
Epoch 3 -- batch 0 / 15 loss: 0.12305469065904617
Epoch 4 -- batch 0 / 15 loss: 0.13111387193202972
Epoch 5 -- batch 0 / 15 loss: 0.12598146498203278
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 15 loss: 0.12247951328754425
Epoch 7 -- batch 0 / 15 loss: 0.13278688490390778
Epoch 8 -- batch 0 / 15 loss: 0.13145852088928223
Epoch 9 -- batch 0 / 15 loss: 0.13807398080825806
Epoch 10 -- batch 0 / 15 loss: 0.12934167683124542
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 1
ranks: tensor([193., 230., 181., 205., 180.,   3.,   9.,  18., 137., 124., 126.,  99.,
         80., 190., 187., 183., 227.,  80., 206., 235., 255., 146.,  42.,  56.,
         36., 163., 100., 116., 115., 237., 235., 249., 250.,  18., 220., 134.,
        244., 246., 248., 241., 221., 219., 231., 225., 222.,  46., 241., 241.,
        255., 109.,  44.,  21., 236.,  19., 232., 241., 238.,  19.,  16.,  15.,
        242., 224., 215., 232., 221., 222., 223., 215., 220., 148., 113., 206.,
        198.,  23.,   2.,  54.,   1., 109., 203., 210.,   4.,  38.,  87., 139.,
        214., 113.,  38., 203., 135., 127., 238., 264., 259.,  27., 161.,  80.,
        238., 234., 215.,  82., 132., 229., 133.,  96., 223., 202., 250., 253.,
         80., 257., 233.,  57., 115.,  23.,  23.,  21.,  32., 242., 205., 245.,
        185., 230.,  65., 191., 244.,  25.,  29., 134.,  75.,  48., 196., 198.,
        150.,  39., 160., 202., 126., 191., 244., 258., 244., 161., 166., 167.,
          1.,   1.,   1.,  22., 187., 205., 209., 196., 207., 132., 148.,  35.,
          5.,   1.,   1., 155.,  40., 223., 123., 188., 201., 176., 134.,   3.,
         96.,  80., 231., 229., 114., 118., 132.,  26.,  91., 178.,  14.,  13.,
          1.,  56.,  12., 103.,  13., 216.,  95., 109.,  13.,  31., 268., 201.,
        254., 232., 185., 224.,   1.,   1., 199., 165., 215., 180., 134., 135.,
        126., 221., 153., 230.,  47.,  23., 230., 198., 179., 194.,  74., 179.,
          1.,  75.,  16.,  17., 141., 230., 166., 209., 188., 229., 234., 237.,
        225., 258., 142.,   8.,  18., 248., 156., 241., 246., 230., 245., 230.,
        243., 243.,  80.,  87.,  17., 174., 237., 229., 233., 246., 228., 243.,
        164., 206., 167., 102.,  83., 149., 102.,  28., 232., 156., 243.,  88.,
         42.,  22.,  33.,  87., 243.,  62., 218., 223., 217.,   9., 223., 206.,
        220.,  78., 175.,  39., 208., 195.,  74.,   7., 107.,  83.,  21., 120.,
        236.,   1., 187., 135.,  76., 200., 226., 256., 243., 205.,   8.,   1.,
          1., 217., 144.,   1., 191., 184., 168., 144., 179., 183., 246., 208.,
        198., 236., 235., 209., 117., 207., 201., 253., 142., 252., 205., 135.,
         36.,  34., 125., 196., 212., 190., 207., 200., 183., 220., 133.,   1.,
        174., 225.,  46., 223., 228., 236.,  49., 185., 177., 115., 203., 236.,
        232., 183., 195., 120., 196.,  82.,  30.,  29.,  48.,  44.,  36., 161.,
        230., 232., 241., 230., 206., 210., 193., 190., 227., 137., 209.,   5.,
         79., 181., 176.,  11., 241.,  62.,  92.,  89., 202.,   5., 134., 248.,
        216.,  66., 260., 211., 199.,  46., 255., 128., 111., 163., 198., 187.,
         68.,  98., 115., 125., 121., 184., 147., 217., 129., 116., 164., 192.,
         80., 202., 202., 170.,  77., 203., 247., 193., 156., 127.,   1., 204.,
        207.,  18.,  15., 176., 209., 158.,   7.,  76., 249., 255., 192., 211.,
        222., 245., 250.,   5., 254.,  13., 227., 128., 135., 255.,  25.,  75.,
        145., 169.,  17.,  16., 191.,   1., 188., 188., 219., 221., 192.,  92.,
          9.,   1., 109.,   7., 135.,  44.,  54.,  66., 211., 209.,  97., 129.,
        115., 131., 174., 197., 208.,  60.,  29.,  31.,  13.,  36., 104., 185.,
        202., 191., 185., 185., 164.,  13., 202.,  12., 201., 187.,  24.,  14.,
        190.,  19., 202., 173., 122., 208., 234., 196., 243.,   1.,   8., 209.,
        219., 215., 197., 224.,  24., 217., 212., 171., 203., 103.,  91., 189.,
        125.,  84., 124.,  36.,  16., 161.,  53., 163., 174., 207., 115., 155.,
        120., 133.,   2.,   5.,   8., 225., 216., 196., 114., 204., 126.,   9.,
        228., 231., 202., 202.,  11., 192.,  79.,  25.,   9., 237., 222., 232.,
         10., 204., 226., 168., 191., 193., 246., 246.,  34., 205., 192.,  86.,
        227., 246., 237.,  86.,  62., 200., 238., 193.,  72., 185., 120.,  38.,
         12., 163., 124.,  66., 169.,   1., 108., 166., 186., 165., 189., 174.,
          3.,   1., 173., 196., 189.,  17.,  95., 199.,  16.,  12.,  56.,   4.,
         86.,  28.,  16., 154.,  50.,  63., 150., 154., 201., 128.,   1., 150.,
          1., 140.,  93.,  28., 110., 135.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1., 155.,   1., 135.,   1.,  73.,  58.,   1.,   1.,   1.,   1.,
        144.,  93.,   1.,   1.,  69., 108.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 136.87576293945312
mrr: 0.09397529810667038
h1: 0.07515337318181992
h3: 0.08282208442687988
h5: 0.09355828166007996
h10: 0.11349692940711975
Testing (cite this): dataloader for dataset Kinships
ranks: tensor([193., 230., 181.,  ..., 180., 183.,  42.])
mr: 133.8104705810547
mrr: 0.122447669506073
h1: 0.1029069796204567
h3: 0.11337209492921829
h5: 0.12383721023797989
h10: 0.15465116500854492
==================================

Done Testing!
done with training and eval
Experiments took 90 seconds
