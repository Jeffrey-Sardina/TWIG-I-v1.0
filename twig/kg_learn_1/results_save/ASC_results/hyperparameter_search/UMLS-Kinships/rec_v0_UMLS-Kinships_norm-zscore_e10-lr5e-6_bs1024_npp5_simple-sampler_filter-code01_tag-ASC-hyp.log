['run_exp.py', '0', 'UMLS-Kinships', '10', '5e-6', 'zscore', '1024', '5', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
Kinships
X_p: torch.Size([8544, 37])
X_p: torch.Size([1074, 37])
X_p: torch.Size([1068, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 15 loss: 0.1318175494670868
Epoch 2 -- batch 0 / 15 loss: 0.13600172102451324
Epoch 3 -- batch 0 / 15 loss: 0.12926234304904938
Epoch 4 -- batch 0 / 15 loss: 0.12464415282011032
Epoch 5 -- batch 0 / 15 loss: 0.1275700330734253
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 15 loss: 0.1359359472990036
Epoch 7 -- batch 0 / 15 loss: 0.13029026985168457
Epoch 8 -- batch 0 / 15 loss: 0.12916016578674316
Epoch 9 -- batch 0 / 15 loss: 0.13424351811408997
Epoch 10 -- batch 0 / 15 loss: 0.12818054854869843
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 1
ranks: tensor([184., 230., 172., 201., 190.,  22.,  38.,  64., 121., 171., 185., 141.,
        144., 203., 193., 189., 230.,  97., 208., 232., 254., 129.,  95.,  83.,
         52., 162.,  75., 105., 126., 241., 239., 249., 250.,  22., 222., 136.,
        249., 245., 243., 242., 224., 222., 229., 229., 224.,  31., 243., 241.,
        255.,  89.,  76.,  16., 230.,  16., 236., 241., 241.,  16.,  14.,  17.,
        243., 232., 211., 201., 151., 222., 225., 217., 220., 162., 128., 141.,
        161.,  18.,   2.,  14.,   1., 123., 203., 185.,   6.,  36.,  85., 150.,
        214.,  78.,  22., 203., 135., 128., 240., 261., 260.,  21., 165.,  57.,
        239., 240., 225.,  82., 145., 236., 129., 117., 220., 207., 250., 253.,
         71., 257., 231.,  27.,  66.,  18.,  19.,  10.,  23., 247., 229., 249.,
         59., 237.,  72., 216., 246.,  32.,  27., 145., 116.,  60., 220., 208.,
        198.,  40., 161., 209., 133., 230., 244., 258., 235., 176., 138., 119.,
          1.,   1.,   1.,  25., 193., 205., 210., 201., 207., 116., 177., 125.,
          6.,   1.,   5., 144., 125., 223., 119., 106., 205., 183., 190.,   9.,
         95.,  95., 230., 229.,  47., 138.,  47.,  19.,  58., 179.,  18.,  14.,
          9.,  57.,  16., 110.,   9., 219., 159., 166.,   9.,  49., 268., 210.,
        254., 242., 193., 227.,   1.,  19., 214., 160., 232., 189., 140., 135.,
        132., 225., 160., 230.,  61.,  23., 228., 198., 187., 186.,  84., 188.,
          1., 140.,  13.,  18., 142., 230., 170., 201., 200., 229., 230., 241.,
        234., 258.,  99.,  12.,  15., 247., 209., 237., 249., 231., 244., 234.,
        248., 243., 109., 103.,  14., 214., 237., 230., 235., 246., 232., 246.,
        175., 202., 149.,  92.,  47., 107.,  53.,  13., 236., 171., 243.,  94.,
         13.,  16.,  30.,  97., 243.,  73., 217., 223., 222.,  10., 223., 202.,
        213.,  38., 102.,  30., 186., 202.,  57.,   5., 136., 110.,  29., 124.,
        236.,   3., 170., 135.,  69., 207., 224., 256., 243., 140.,   9.,  10.,
          3., 208.,  21.,   1., 227., 201., 157., 135., 178., 220., 240., 205.,
        205., 225., 231., 202., 116., 202., 216., 253., 155., 252., 203., 135.,
         35.,  30.,  94., 201., 212., 171., 207., 198., 176., 217., 156.,   1.,
        160., 223.,  51., 223., 230., 234.,  66., 177., 137., 125., 212., 236.,
        236., 188., 173., 115., 194.,  83.,  24.,  28.,  29.,  24.,  33., 162.,
        232., 237., 239., 233., 227., 227., 218., 214., 235., 117., 226.,   1.,
         40., 198., 142.,   9., 241.,  39., 100.,  89., 223.,   9., 130., 249.,
        199.,  79., 260., 210., 150.,  45., 255.,  56., 106., 172., 179., 154.,
         32., 112., 120., 129., 128., 192., 165., 216.,  26.,  31., 191., 217.,
        165., 219., 224., 203.,  81., 220., 247., 197.,  44., 129.,   1., 215.,
        226.,  20.,  13., 210., 211., 130.,   1.,  92., 249., 256., 152., 222.,
        227., 245., 251.,   6., 253.,  22., 235., 133., 135., 255.,  29., 182.,
        159., 185.,  20.,  20., 191.,   3., 189., 185., 222., 223., 192.,  99.,
          8.,   1., 114.,   6., 143.,  47., 130., 129., 218., 218., 109., 142.,
        120., 127., 190., 194., 200., 112.,  39.,  32.,  12.,  22., 100., 199.,
        213., 181., 165., 189., 173.,  16., 210.,  22., 211., 205.,  26.,  14.,
        208.,  22., 209., 133., 142., 219., 243., 229., 243.,   4.,  29., 222.,
        220., 218., 203., 226.,  26., 222., 219., 188., 212., 107.,  91., 150.,
         67.,  93., 128.,  52.,  26., 180.,  63., 174., 168., 206.,  27., 153.,
         51., 122.,   2.,   5.,  12., 238., 221., 214., 131., 210., 111.,  44.,
        226., 227., 196., 202.,  14., 216.,  60.,  26.,   6., 237., 221., 228.,
         10., 229., 234., 165., 198., 195., 246., 246.,  40., 221., 156., 133.,
        227., 246., 235.,  65., 140., 197., 233., 195.,  60., 104., 142.,  28.,
         18., 158., 126.,  69., 169.,   1., 111., 191., 186., 165., 192., 183.,
          6.,   1., 187., 206., 180.,  16.,  90., 188.,  44.,  46.,  50.,   7.,
         89.,  60.,  57., 147.,  16.,  72., 152., 130., 200., 133.,   1., 112.,
          1., 136.,  97.,  38., 124., 135.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1., 161.,   1., 155.,   1.,  70.,  50.,   1.,   1.,   1.,   1.,
        145., 123.,   1.,   1.,  18., 113.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 137.86810302734375
mrr: 0.08537878096103668
h1: 0.0659509226679802
h3: 0.07361963391304016
h5: 0.07975459843873978
h10: 0.10889570415019989
Testing (cite this): dataloader for dataset Kinships
ranks: tensor([184., 230., 172.,  ..., 180., 183.,  94.])
mr: 140.4072723388672
mrr: 0.08951929956674576
h1: 0.07267441600561142
h3: 0.07906977087259293
h5: 0.08604650944471359
h10: 0.11162790656089783
==================================

Done Testing!
done with training and eval
Experiments took 77 seconds
