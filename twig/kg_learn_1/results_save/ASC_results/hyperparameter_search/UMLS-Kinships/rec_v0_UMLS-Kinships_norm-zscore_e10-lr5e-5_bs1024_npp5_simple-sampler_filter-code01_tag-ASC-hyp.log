['run_exp.py', '0', 'UMLS-Kinships', '10', '5e-5', 'zscore', '1024', '5', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
Kinships
X_p: torch.Size([8544, 37])
X_p: torch.Size([1074, 37])
X_p: torch.Size([1068, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 15 loss: 0.13144904375076294
Epoch 2 -- batch 0 / 15 loss: 0.1336033046245575
Epoch 3 -- batch 0 / 15 loss: 0.12795646488666534
Epoch 4 -- batch 0 / 15 loss: 0.12354469299316406
Epoch 5 -- batch 0 / 15 loss: 0.12779371440410614
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 15 loss: 0.13587205111980438
Epoch 7 -- batch 0 / 15 loss: 0.12687377631664276
Epoch 8 -- batch 0 / 15 loss: 0.12744922935962677
Epoch 9 -- batch 0 / 15 loss: 0.13035894930362701
Epoch 10 -- batch 0 / 15 loss: 0.12788331508636475
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 1
ranks: tensor([192., 230., 179., 205., 180.,   3.,   9.,  23., 148., 121., 143.,  91.,
         84., 189., 187., 184., 226.,  84., 204., 240., 255., 144.,  54.,  63.,
         42., 162.,  83., 110., 114., 238., 232., 249., 250.,  26., 217., 135.,
        246., 242., 242., 240., 211., 217., 229., 224., 225.,  43., 240., 241.,
        255., 111.,  34.,  19., 232.,  17., 230., 241., 241.,  20.,  14.,  17.,
        242., 227., 217., 223., 221., 222., 220., 215., 220., 144., 114., 197.,
        203.,  23.,   1.,  53.,   1., 104., 203., 210.,   2.,  38.,  88., 130.,
        214.,  98.,  31., 197., 135., 128., 235., 262., 260.,  22., 165.,  74.,
        235., 235., 222.,  81., 132., 232., 129.,  99., 221., 195., 250., 253.,
         72., 257., 237.,  53., 120.,  25.,  26.,  22.,  30., 244., 206., 246.,
        182., 224.,  60., 196., 244.,  30.,  26., 128.,  74.,  45., 200., 202.,
        170.,  37., 163., 202., 132., 199., 245., 258., 239., 150., 163., 162.,
          1.,   1.,   1.,  24., 190., 204., 210., 199., 207., 144., 141.,  42.,
          5.,   1.,   1., 154.,  47., 223., 121., 187., 199., 179., 141.,   8.,
         97.,  81., 231., 229.,  99., 117., 138.,  26.,  90., 184.,  20.,  15.,
          2.,  55.,  12., 110.,   9., 213., 117., 122.,   9.,  42., 268., 206.,
        254., 231., 182., 220.,   1.,   1., 195., 162., 216., 182., 124., 135.,
        130., 222., 150., 230.,  58.,  21., 230., 198., 179., 186.,  76., 187.,
          1.,  78.,  11.,  17., 142., 230., 173., 209., 181., 228., 234., 235.,
        229., 258., 136.,   9.,  24., 247., 152., 242., 247., 231., 240., 234.,
        246., 243.,  79.,  79.,  13., 175., 234., 227., 231., 246., 227., 240.,
        171., 205., 168., 100.,  82., 142., 108.,  25., 232., 159., 243.,  91.,
         35.,  25.,  29.,  91., 243.,  63., 218., 223., 222.,   9., 223., 208.,
        215.,  76., 177.,  36., 210., 193.,  65.,   6.,  99.,  94.,  17., 120.,
        236.,   1., 177., 135.,  77., 200., 229., 256., 243., 203.,   7.,   1.,
          1., 216., 141.,   1., 200., 189., 161., 148., 172., 195., 247., 209.,
        205., 236., 235., 206., 117., 202., 195., 253., 139., 252., 203., 135.,
         33.,  30., 115., 202., 212., 188., 207., 199., 175., 217., 125.,   3.,
        173., 225.,  55., 223., 228., 236.,  59., 174., 166., 106., 203., 236.,
        231., 185., 203., 129., 196.,  86.,  28.,  28.,  61.,  41.,  35., 164.,
        230., 233., 239., 226., 210., 205., 198., 184., 228., 119., 210.,   5.,
         76., 192., 165.,  12., 241.,  58.,  91.,  88., 206.,   5., 131., 247.,
        204.,  76., 260., 209., 198.,  40., 255., 120.,  99., 173., 196., 183.,
         55., 101., 107., 127., 120., 186., 151., 219., 130., 125., 169., 188.,
         75., 192., 201., 166.,  79., 202., 246., 195., 153., 129.,   1., 201.,
        209.,  16.,  14., 182., 211., 154.,   4.,  76., 249., 255., 187., 213.,
        214., 245., 251.,   5., 254.,  17., 229., 129., 135., 255.,  25.,  84.,
        145., 162.,  16.,  15., 191.,   1., 186., 184., 221., 221., 192.,  89.,
          9.,   1., 109.,   6., 131.,  39.,  58.,  59., 202., 205.,  94., 131.,
        121., 126., 172., 200., 213.,  50.,  34.,  30.,  12.,  29., 107., 184.,
        202., 189., 182., 186., 170.,  15., 201.,  14., 199., 186.,  21.,  14.,
        191.,  19., 201., 171., 120., 209., 238., 203., 243.,   1.,  10., 207.,
        215., 216., 203., 224.,  25., 217., 213., 175., 202., 102.,  89., 196.,
        124.,  88., 124.,  42.,  13., 161.,  54., 169., 175., 206., 108., 157.,
        108., 134.,   2.,   5.,   9., 225., 217., 198., 118., 204., 132.,  13.,
        228., 229., 198., 202.,  13., 195.,  73.,  24.,   9., 237., 221., 228.,
          9., 207., 224., 170., 187., 193., 246., 246.,  36., 204., 197.,  90.,
        222., 246., 237., 116.,  68., 198., 238., 195.,  70., 182., 124.,  46.,
         10., 168., 126.,  72., 169.,   1., 111., 184., 184., 169., 192., 172.,
          6.,   1., 171., 197., 182.,  21.,  90., 198.,  13.,  13.,  56.,   7.,
         81.,  25.,  23., 157.,  41.,  59., 145., 152., 185., 133.,   1., 152.,
          1., 135.,  90.,  32., 110., 135.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1., 156.,   1., 132.,   1.,  77.,  51.,   1.,   1.,   1.,   1.,
        145.,  99.,   1.,   1.,  77., 106.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 136.73619079589844
mrr: 0.09282474964857101
h1: 0.07361963391304016
h3: 0.08128834515810013
h5: 0.09049079567193985
h10: 0.11656441539525986
Testing (cite this): dataloader for dataset Kinships
ranks: tensor([192., 230., 179.,  ..., 180., 183.,  43.])
mr: 134.2657012939453
mrr: 0.11813416332006454
h1: 0.09825581312179565
h3: 0.1093023270368576
h5: 0.11802325397729874
h10: 0.14883720874786377
==================================

Done Testing!
done with training and eval
Experiments took 74 seconds
