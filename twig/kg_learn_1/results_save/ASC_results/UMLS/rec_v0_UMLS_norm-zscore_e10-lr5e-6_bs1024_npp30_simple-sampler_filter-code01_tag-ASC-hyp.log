['run_exp.py', '0', 'UMLS', '10', '5e-6', 'zscore', '1024', '30', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 6 loss: 0.13202832639217377
Epoch 2 -- batch 0 / 6 loss: 0.13376080989837646
Epoch 3 -- batch 0 / 6 loss: 0.1275940239429474
Epoch 4 -- batch 0 / 6 loss: 0.1290043741464615
Epoch 5 -- batch 0 / 6 loss: 0.13629122078418732
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 6 loss: 0.12785059213638306
Epoch 7 -- batch 0 / 6 loss: 0.13147898018360138
Epoch 8 -- batch 0 / 6 loss: 0.13069170713424683
Epoch 9 -- batch 0 / 6 loss: 0.128087118268013
Epoch 10 -- batch 0 / 6 loss: 0.13089393079280853
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 1
ranks: tensor([184., 230., 170., 202., 189.,  25.,  30.,  52., 104., 165., 174., 139.,
        136., 200., 194., 190., 230.,  85., 203., 226., 253., 127.,  93.,  66.,
         51., 163.,  73.,  94., 127., 244., 245., 249., 250.,  19., 225., 134.,
        250., 244., 247., 240., 230., 222., 229., 232., 229.,  34., 243., 240.,
        255.,  67.,  84.,  11., 231.,  15., 235., 242., 236.,   8.,  19.,  17.,
        243., 235., 210., 216., 175., 222., 226., 217., 220., 160., 128., 139.,
        160.,  24.,   3.,   7.,   1., 135., 203., 196.,   4.,  39.,  77., 147.,
        214.,  77.,  29., 207., 134., 130., 241., 261., 259.,  25., 161.,  70.,
        240., 241., 218.,  92., 135., 237., 133., 119., 217., 218., 250., 253.,
         77., 257., 227.,  29.,  60.,  15.,  10.,  11.,  18., 249., 238., 252.,
         65., 241.,  76., 223., 245.,  27.,  22., 160., 124.,  70., 216., 207.,
        202.,  49., 161., 206., 128., 230., 246., 258., 240., 180., 148., 121.,
          1.,   1.,   1.,  28., 196., 211., 210., 200., 207.,  68., 174.,  82.,
          7.,   1.,   1., 128., 105., 223., 120.,  93., 216., 186., 185.,   9.,
         89.,  97., 230., 229.,  57., 143.,  38.,  20.,  69., 186.,  16.,  12.,
          7.,  55.,  16., 106.,  13., 219., 141., 158.,  13.,  49., 268., 214.,
        254., 245., 194., 235.,   1.,  17., 214., 160., 234., 186., 142., 135.,
        132., 224., 133., 230.,  33.,  15., 226., 198., 186., 188.,  98., 181.,
          1., 137.,  12.,  18., 146., 230., 166., 191., 199., 230., 230., 241.,
        237., 258.,  92.,   4.,  22., 249., 221., 237., 250., 230., 243., 238.,
        245., 243., 116., 124.,  14., 217., 239., 229., 238., 246., 235., 247.,
        164., 207., 139.,  95.,  33.,  97.,  32.,   7., 235., 168., 243.,  95.,
         12.,  14.,  26., 102., 243.,  68., 222., 223., 219.,   9., 223., 204.,
        215.,  45.,  94.,  34., 190., 201.,  55.,   5., 139., 101.,  36., 116.,
        236.,   1., 164., 135.,  60., 204., 214., 256., 243., 143.,  10.,   6.,
          6., 204.,  29.,   1., 231., 197., 169., 145., 178., 211., 238., 202.,
        200., 219., 231., 211., 114., 199., 227., 253., 156., 252., 204., 135.,
         35.,  27., 103., 194., 212., 182., 207., 198., 171., 220., 134.,   1.,
        162., 224.,  26., 223., 230., 232.,  60., 187., 130., 146., 211., 236.,
        237., 186., 148.,  92., 194.,  81.,  27.,  26.,  24.,  24.,  34., 158.,
        231., 239., 241., 236., 226., 231., 229., 217., 237., 134., 226.,   1.,
         36., 209., 142.,   9., 241.,  36., 101.,  90., 228.,   1., 132., 248.,
        220.,  74., 260., 201., 142.,  49., 255.,  56., 121., 160., 163., 139.,
         22., 126., 131., 129., 128., 200., 182., 216.,  25.,  32., 197., 223.,
        157., 220., 221., 206.,  79., 228., 248., 202.,  56., 133.,   1., 218.,
        228.,  25.,  17., 210., 212., 127.,   1.,  70., 249., 256., 154., 228.,
        234., 245., 251.,   6., 254.,  21., 235., 134., 135., 255.,  30., 182.,
        165., 194.,  29.,  15., 191.,   9., 191., 196., 222., 222., 192., 111.,
          4.,   1., 128.,   7., 162.,  51., 112., 122., 217., 218., 111., 139.,
        118., 131., 185., 196., 196., 106.,  35.,  25.,  10.,  21.,  92., 199.,
        215., 183., 177., 190., 168.,  14., 211.,  27., 211., 207.,  25.,  15.,
        204.,  21., 214., 141., 160., 220., 243., 223., 243.,   5.,  19., 218.,
        221., 218., 205., 226.,  26., 223., 220., 196., 212., 110.,  82., 131.,
         67.,  91., 130.,  49.,  23., 183.,  64., 153., 180., 203.,  41., 155.,
         79., 123.,   2.,   6.,  12., 240., 222., 216., 131., 205.,  91.,  30.,
        223., 229., 204., 201.,  15., 221.,  47.,  32.,   3., 237., 224., 234.,
         11., 233., 235., 164., 201., 194., 246., 246.,  39., 220., 159., 126.,
        225., 246., 235.,  36., 115., 193., 230., 189.,  58.,  83., 142.,  24.,
         13., 151., 126.,  58., 169.,   1., 104., 184., 196., 165., 188., 188.,
          4.,   1., 192., 206., 189.,  14.,  88., 186.,  33.,  39.,  50.,   5.,
         87.,  53.,  44., 145.,  14.,  77., 155., 126., 209., 133.,   1., 111.,
          1., 137., 105.,  42., 123., 135.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1., 162.,   1., 145.,   1.,  65.,  58.,   1.,   1.,   1.,   1.,
        147., 128.,   1.,   1.,  17., 121.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 137.4662628173828
mrr: 0.08973480015993118
h1: 0.07055214792490005
h3: 0.07515337318181992
h5: 0.08588957041501999
h10: 0.11196319013834
==================================

Done Testing!
done with training and eval
Experiments took 39 seconds
