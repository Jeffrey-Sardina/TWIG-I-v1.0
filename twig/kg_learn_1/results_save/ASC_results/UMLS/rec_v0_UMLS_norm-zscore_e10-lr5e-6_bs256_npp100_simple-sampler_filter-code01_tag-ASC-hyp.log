['run_exp.py', '0', 'UMLS', '10', '5e-6', 'zscore', '256', '100', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 21 loss: 0.1346409171819687
Epoch 2 -- batch 0 / 21 loss: 0.12700240314006805
Epoch 3 -- batch 0 / 21 loss: 0.14400528371334076
Epoch 4 -- batch 0 / 21 loss: 0.13882751762866974
Epoch 5 -- batch 0 / 21 loss: 0.13173307478427887
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 21 loss: 0.13594460487365723
Epoch 7 -- batch 0 / 21 loss: 0.1258031576871872
Epoch 8 -- batch 0 / 21 loss: 0.13370347023010254
Epoch 9 -- batch 0 / 21 loss: 0.14152514934539795
Epoch 10 -- batch 0 / 21 loss: 0.12864427268505096
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 3
ranks: tensor([184., 230., 172., 203., 182.,  11.,  19.,  30., 102., 156., 153., 115.,
        127., 197., 188., 177., 230.,  78., 205., 229., 254., 126.,  92.,  57.,
         53., 165.,  80.,  92., 121., 247., 240., 249., 250.,  13., 228., 127.,
        248., 245., 246., 242., 232., 220., 229., 231., 231.,  34., 241., 243.,
        255.,  79.,  83.,  10., 233.,  18., 238., 243., 237.,  14.,  17.,  19.,
        243., 235., 214., 225., 219., 222., 227., 220., 220., 153., 145., 190.,
        201.,  19.,   2.,  12.,   1., 120., 203., 206.,   6.,  33.,  84., 148.,
        215.,  70.,  24., 210., 135., 128., 243., 264., 259.,  26., 167.,  65.,
        240., 240., 211.,  93., 140., 236., 131., 124., 216., 220., 250., 253.,
         83., 257., 232.,  28.,  69.,  17.,  20.,  15.,  23., 247., 238., 250.,
        116., 238.,  98., 197., 246.,  26.,  26., 154., 107.,  59., 218., 209.,
        189.,  40., 161., 207., 131., 218., 245., 258., 243., 188., 154., 141.,
          1.,   1.,   1.,  29., 196., 211., 211., 200., 207.,  89., 166.,  46.,
          2.,   1.,   1., 137.,  77., 223., 122., 113., 209., 189., 178.,  12.,
        104.,  85., 231., 229.,  58., 136.,  56.,  21.,  84., 193.,  18.,  12.,
          8.,  63.,  17.,  99.,   6., 220., 121., 131.,   9.,  41., 268., 213.,
        254., 244., 195., 229.,   1.,   3., 210., 158., 228., 191., 144., 135.,
        131., 225., 130., 230.,  28.,   5., 223., 198., 191., 180.,  94., 191.,
          1., 120.,  14.,  18., 146., 230., 163., 191., 193., 233., 232., 239.,
        239.,  76., 109.,   3.,  18., 247., 212., 243., 164., 141.,   1.,   1.,
          1., 243.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1., 233., 157., 162.,  66., 242., 234., 243., 201.,
         46., 222., 180., 121., 243., 222., 222., 223., 206.,   3., 215., 121.,
         74.,  29.,  54.,   9.,  88.,  48., 101.,   5.,  96.,  88., 123.,  48.,
        232.,   4.,  22., 135., 208., 240., 237., 254., 236., 220.,  82.,  41.,
         35., 232., 208.,  20., 236., 233., 234., 232., 242., 240., 235., 155.,
         28., 227., 229., 201.,   6.,  34., 146., 253., 154., 252., 202., 135.,
         91.,   7.,  10.,  85., 211., 178., 204., 129.,  94., 213., 148.,  68.,
        174., 224., 104., 222., 193., 234.,  87., 156.,  57., 168., 146., 234.,
        224., 152., 195., 205., 219., 133., 210., 210., 218., 203., 203., 210.,
        203., 164., 236.,  39.,  30.,  30.,  31.,  30., 103.,   1.,  36.,   7.,
          8.,  20.,   8.,   1., 239., 245., 134., 115., 230.,   1., 131., 182.,
        206., 122., 259., 189., 126., 125., 255., 116., 190.,  62.,  25.,  35.,
        132., 204., 118., 119., 119.,   7.,  11., 202., 135., 178., 216., 222.,
        197., 214., 211., 165., 217., 216., 246., 237.,  17., 125.,   9., 249.,
        138.,  21., 135., 209., 176., 188.,  86., 120., 256., 256., 200., 244.,
        248., 243., 256.,   1., 256.,   1., 182., 125., 135., 255., 136., 194.,
        164., 194.,  12.,   4., 191.,  19., 195., 190., 223., 223., 153.,   5.,
         23.,   4.,  12.,  17.,  24.,   5., 107., 124., 218., 165., 140., 162.,
         83.,  83.,  38., 179., 100.,  71.,  13.,   1.,   1.,   8., 188., 198.,
        209., 198., 185., 198., 161.,  16., 210.,   4., 206., 205., 128.,  43.,
        205., 129., 207., 147., 133., 219.,  54.,   1.,   1.,   1.,   1.,   1.,
          1., 219., 204., 226.,  20., 217., 213.,  66.,  85.,  99.,  83., 136.,
        144., 131., 115.,  18., 128., 157., 183., 185., 190., 212., 166., 176.,
        126., 131.,   1.,   1.,   1., 242., 215., 192., 196., 187., 148., 106.,
        228., 229., 228., 219., 110., 215.,  18.,  10., 113., 237., 206., 239.,
         34., 226., 237., 216., 228., 237., 246., 246., 198., 241., 168.,  86.,
        225., 246., 238.,  28.,  68., 194., 236., 202.,   7.,  29.,  32., 133.,
        150.,  36., 131., 107., 169.,  82., 113.,  84., 187., 115., 169., 177.,
         19.,   7., 173., 200., 185.,   1.,   1., 197., 117., 119., 112., 100.,
        123.,  85., 115., 229.,   1.,  19., 212.,  40.,  20., 217., 178., 239.,
        254., 152.,   8.,   7.,  84., 135., 249., 253.,  14.,  21., 251., 249.,
          1., 134., 148., 129., 200.,  12.,   5.,   3.,   1.,   1.,   1.,   1.,
         45.,  99.,   1.,   1.,  25., 123., 152.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 133.79600524902344
mrr: 0.11332134157419205
h1: 0.0920245423913002
h3: 0.10122699290513992
h5: 0.1150306761264801
h10: 0.14570552110671997
==================================

Done Testing!
done with training and eval
Experiments took 48 seconds
