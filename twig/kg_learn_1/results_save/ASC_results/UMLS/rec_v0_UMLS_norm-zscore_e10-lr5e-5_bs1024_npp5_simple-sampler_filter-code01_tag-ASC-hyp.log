['run_exp.py', '0', 'UMLS', '10', '5e-5', 'zscore', '1024', '5', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 6 loss: 0.13129810988903046
Epoch 2 -- batch 0 / 6 loss: 0.13206081092357635
Epoch 3 -- batch 0 / 6 loss: 0.1265939474105835
Epoch 4 -- batch 0 / 6 loss: 0.12493075430393219
Epoch 5 -- batch 0 / 6 loss: 0.13509511947631836
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 6 loss: 0.12445477396249771
Epoch 7 -- batch 0 / 6 loss: 0.1275234967470169
Epoch 8 -- batch 0 / 6 loss: 0.1271212100982666
Epoch 9 -- batch 0 / 6 loss: 0.1247473955154419
Epoch 10 -- batch 0 / 6 loss: 0.12773047387599945
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 1
ranks: tensor([193., 230., 178., 207., 169.,   1.,   1.,   6.,  81.,  99., 106.,  69.,
         77., 184., 181., 184., 225.,  42., 195., 233., 254., 131.,  87.,  47.,
         59., 164.,  86.,  79., 112., 245., 239., 249., 250.,  23., 230., 129.,
        250., 249., 248., 240., 227., 223., 232., 232., 232.,  36., 241., 243.,
        255.,  89.,  79.,  17., 236.,  17., 231., 243., 236.,  15.,  18.,  15.,
        243., 229., 211., 233., 227., 222., 223., 215., 220., 151., 178., 200.,
        206.,  30.,   1.,  19.,   1., 121., 203., 210.,   5.,  34.,  84., 132.,
        213.,  85.,  29., 207., 135., 131., 242., 264., 259.,  24., 164.,  69.,
        240., 242., 216.,  90., 132., 231., 143., 116., 211., 226., 250., 253.,
        113., 257., 237.,  37., 112.,  18.,  18.,  19.,  31., 250., 229., 251.,
        200., 240.,  89., 188., 248.,  27.,  29., 143., 109.,  57., 222., 219.,
        162.,  34., 159., 203., 127., 192., 246., 258., 248., 173., 163., 171.,
          1.,   1.,   1.,  23., 200., 212., 211., 200., 207.,  65.,  93.,   9.,
          1.,   1.,   1., 134.,  21., 223., 119., 164., 209., 200., 165.,   9.,
        121.,  87., 231., 229.,  92., 131.,  99.,  23.,  96., 189.,  17.,  11.,
          3.,  44.,  13., 103.,  11., 213.,  80.,  87.,  11.,  37., 268., 217.,
        254., 235., 186., 226.,   1.,   1., 193., 161., 213., 183., 141., 135.,
        131., 222., 120., 230.,   6.,   1., 225., 198., 186., 182.,  98., 188.,
          1.,  66.,  13.,  15., 154., 230., 175., 177., 189., 236., 236., 240.,
        235., 258., 123.,   1.,  26., 249., 204., 242., 247., 223., 246., 239.,
        243., 243.,  91.,  81.,  11., 204., 240., 230., 239., 246., 230., 241.,
        171., 207., 160.,  99.,  79., 159.,  91.,   7., 238., 161., 243.,  91.,
         26.,  13.,  28.,  95., 243.,  60., 221., 223., 220.,   9., 223., 206.,
        218.,  63., 139.,  35., 194., 201.,  58.,   5., 107.,  95.,  17., 129.,
        236.,   1., 183., 135.,  79., 192., 198., 256., 243., 212.,   8.,   6.,
          4., 207.,  43.,   1., 221., 202., 181., 154., 177., 186., 242., 201.,
        198., 225., 241., 228., 116., 204., 208., 253., 150., 252., 203., 135.,
         31.,  27., 117., 197., 212., 192., 207., 205., 177., 214.,  43.,  12.,
        168., 227.,  14., 223., 236., 236.,  64., 182., 149., 126., 210., 236.,
        232., 204., 189., 114., 192.,  89.,  30.,  28.,  26.,  34.,  38., 160.,
        234., 236., 240., 238., 213., 220., 214., 204., 233., 132., 214.,   1.,
         59., 187., 148.,   8., 241.,  41.,  92.,  85., 223.,   1., 134., 248.,
        242.,  76., 260., 182., 180.,  42., 255., 144., 125., 173., 183., 183.,
         28., 159., 122., 127., 127., 184., 152., 217.,  46.,  51., 184., 211.,
         61., 208., 217., 128.,  78., 228., 247., 203., 163., 130.,   1., 199.,
        220.,  19.,  16., 184., 210., 160.,   1.,  38., 250., 256., 216., 228.,
        232., 245., 251.,   4., 256.,  17., 229., 132., 135., 255.,  30., 177.,
        167., 187.,  24.,  24., 191.,   4., 193., 197., 223., 221., 192., 108.,
          7.,   1., 114.,   6., 131.,  42.,  53.,  41., 204., 206., 103., 132.,
        120., 128., 168., 201., 209.,  32.,  31.,   5.,   8.,  25., 104., 195.,
        212., 191., 196., 196., 167.,  16., 207.,  18., 203., 197.,  22.,  12.,
        192.,  22., 201., 185., 131., 217., 238., 199., 243.,   1.,   7., 200.,
        211., 221., 211., 226.,  22., 222., 217., 181., 213., 115.,  86., 131.,
        100.,  81., 129.,  42.,  11., 173.,  61., 162., 179., 209., 144., 157.,
        155., 125.,   1.,   4.,   7., 225., 215., 196., 148., 188.,  90.,   6.,
        230., 227., 206., 202.,  14., 207.,  59.,  32.,   7., 237., 230., 238.,
         11., 221., 224., 174., 181., 196., 246., 246.,  41., 217., 160., 119.,
        228., 246., 238.,  13.,  26., 192., 235., 192.,  58., 133., 130.,  16.,
          6., 168., 120.,  71., 169.,   1., 110., 152., 195., 180., 196., 175.,
          3.,   1., 181., 201., 190.,  16.,  94., 189.,  12.,  10.,  50.,   6.,
         79.,  27.,   9., 142.,  24.,  73., 137., 149., 204., 131.,   1., 161.,
          1., 153., 103.,  34., 119., 135.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1., 149.,   1., 127.,   1.,  70.,  53.,   1.,   1.,   1.,   1.,
        143.,  75.,   1.,   1.,  22., 115.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 136.38650512695312
mrr: 0.10177511721849442
h1: 0.08282208442687988
h3: 0.08588957041501999
h5: 0.09662576764822006
h10: 0.12730062007904053
==================================

Done Testing!
done with training and eval
Experiments took 37 seconds
