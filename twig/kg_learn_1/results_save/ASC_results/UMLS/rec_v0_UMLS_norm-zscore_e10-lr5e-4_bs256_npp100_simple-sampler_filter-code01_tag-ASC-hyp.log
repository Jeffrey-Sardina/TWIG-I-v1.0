['run_exp.py', '0', 'UMLS', '10', '5e-4', 'zscore', '256', '100', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 21 loss: 0.13508084416389465
Epoch 2 -- batch 0 / 21 loss: 0.11258290708065033
Epoch 3 -- batch 0 / 21 loss: 0.1148151382803917
Epoch 4 -- batch 0 / 21 loss: 0.09250569343566895
Epoch 5 -- batch 0 / 21 loss: 0.07898208498954773
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 21 loss: 0.08943048119544983
Epoch 7 -- batch 0 / 21 loss: 0.07108256220817566
Epoch 8 -- batch 0 / 21 loss: 0.07477159053087234
Epoch 9 -- batch 0 / 21 loss: 0.08043910562992096
Epoch 10 -- batch 0 / 21 loss: 0.06802453845739365
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 3
ranks: tensor([115., 154.,  12.,  32.,   8.,   1.,   1.,   1.,   9.,   8.,  10.,   5.,
         11.,   9.,  10.,   9., 100.,  10.,   3., 207., 255.,  57.,  24.,   1.,
         76., 146.,  54.,   6.,  42., 198., 247., 110., 250.,  98., 240., 139.,
        186., 205., 251.,  23., 134., 138.,  94., 118.,  91.,  72., 241., 237.,
        255.,  14.,  15.,   5., 146.,   6., 100., 158., 243.,   7.,   8.,   7.,
         36.,   6., 204.,  75., 189., 222.,  12., 164., 190.,  12.,   7.,  73.,
         96.,   2.,   1.,   5.,   1.,  27.,  73.,  61.,   1.,   3.,  13.,  45.,
         82.,  75.,   7., 149., 133., 105., 148., 264., 253.,  27.,  38.,  42.,
        197., 131., 172.,  37.,   1.,  16.,  30.,  42., 195., 250., 250.,  19.,
        232., 257.,  67.,   1.,   9.,   1.,   1.,   1.,   1., 248.,  13., 224.,
          8.,   1.,   1.,  19.,   1.,   1.,   1.,  20.,   7.,  11.,   1.,  22.,
          1.,   1.,  81.,   1.,  22.,   1.,  72., 255., 252.,  60., 195., 181.,
          1.,   1.,   1.,  93., 158., 188., 207.,  68., 113.,   1.,  37.,   4.,
          1.,   2.,   1.,  81.,   7., 220.,  78.,  10., 158., 114.,  22.,   1.,
          1.,   5.,  52., 111.,   1.,  23.,  51.,  16.,   1.,  31.,   1.,   1.,
          1.,  53.,   1.,  16.,   1.,   2.,  11.,  10.,   1.,   6., 150.,  80.,
         95.,  11.,  10.,  74.,   1.,   1.,   1.,  92.,   1.,  18.,   6.,  49.,
         12., 158.,  12.,   6.,   9.,   7., 200., 198., 151., 151., 198., 139.,
          6.,  53.,  44.,   6.,  45., 188.,  10.,  10.,  81., 104., 119., 159.,
        190.,  72.,  52.,   7.,  47., 244.,  19., 174., 165.,   7.,   1.,   1.,
          1.,  77.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.,  33.,  28.,  40., 111.,  39., 140.,  77.,   7.,
         20.,  83., 162., 139., 139., 234., 225.,   8.,   1.,   1.,  23.,  49.,
         35.,  16.,   7.,   3.,  28.,   5.,  29.,   1.,   1.,   8.,   5.,   1.,
         31.,   3.,   3.,  54.,   8.,   3., 157.,  71., 152., 136.,   1.,   1.,
          1., 136.,  75.,   1., 121., 239., 238., 242., 117.,   5., 117.,  43.,
         30., 211., 127., 146.,   1.,  59.,  45.,   8.,   1.,  52.,   1., 135.,
          1.,   1.,   1.,  55.,   7., 130., 203., 193.,   6.,  53.,  94., 139.,
        143., 193.,  94., 142.,  91., 219.,  76., 117.,  38.,  97.,  92., 236.,
        223.,  74., 155., 139., 110.,  12., 104., 114., 223., 185., 134., 219.,
        133.,  63., 117.,  25.,   8.,  18.,   8.,  10.,  17.,   1.,  19.,   1.,
          1.,   1.,   1.,   1., 142., 213.,  89.,  71., 190.,   1.,  80., 157.,
        239.,  94., 256.,  83., 116., 122.,  77., 104., 127.,  19.,  14.,  33.,
         82., 121.,   2.,  10.,  27.,   1.,   1., 129.,  76., 100., 126.,  10.,
          1.,   7.,   9.,   1.,   5.,   1.,  47., 135.,  15., 132.,   1.,  18.,
         27.,   1.,  30.,   1.,   1.,  18.,  59.,  18., 143., 148.,   2.,  80.,
         79., 245.,   7.,   1.,   4.,   1.,  19.,   3.,  32., 228.,  64., 167.,
        208., 194.,   4.,   5.,  87.,  73., 178.,  68.,   2.,   2.,   3.,   1.,
          1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   3.,  69.,   6.,   1.,
         35.,  81.,   1., 119.,  76.,  35.,   1.,   1.,   1.,   4.,   5.,  34.,
         93., 227., 226., 222.,  13.,  36.,  66.,   1.,  80.,  99.,  11.,   1.,
        101.,  29.,  89., 111.,  14.,  88.,   2.,   1.,   1.,   1.,   1.,   1.,
          1., 174., 213., 227.,   5.,   1.,  73.,   9.,   1.,   3.,  84.,   1.,
        166.,  83., 179.,  20.,  61.,  41.,  43.,   1.,  76.,  62.,   2.,   3.,
          1.,   1.,   1.,   1.,   1., 152., 107.,  61.,  32.,  71., 146., 114.,
        234., 234., 172.,  87.,  14., 129.,   1., 181., 203.,  94., 236., 242.,
          1., 121., 138.,  36.,  96., 230., 246., 108., 157., 246., 188., 102.,
        245.,   4., 242.,  15.,  97.,  72., 237.,  93.,  86.,  81., 135.,   7.,
        205., 216., 135., 135., 169.,   1., 135., 101., 210., 169., 203.,  20.,
         21.,   7.,  52.,  11.,  59.,   2.,   1.,   2.,  98., 119., 102.,  37.,
        115.,  79.,  99., 156.,  23.,  17.,  75.,   4.,  45.,  38.,  86., 242.,
        254.,  43.,   1.,   1.,  49.,  98., 243., 249.,  32.,  90., 217., 246.,
          1.,  24., 167.,   3.,  18.,   1.,   6.,   4.,   1.,   1.,   1.,   1.,
          2.,  11.,   1.,   1.,   1.,  39.,  56.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 70.74539947509766
mrr: 0.27115049958229065
h1: 0.2269938588142395
h3: 0.2638036906719208
h5: 0.29141104221343994
h10: 0.3757668733596802
==================================

Done Testing!
done with training and eval
Experiments took 53 seconds
