['run_exp.py', '0', 'UMLS', '10', '5e-6', 'zscore', '256', '30', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 21 loss: 0.13562901318073273
Epoch 2 -- batch 0 / 21 loss: 0.126442089676857
Epoch 3 -- batch 0 / 21 loss: 0.14265792071819305
Epoch 4 -- batch 0 / 21 loss: 0.13908734917640686
Epoch 5 -- batch 0 / 21 loss: 0.13168665766716003
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 21 loss: 0.13589920103549957
Epoch 7 -- batch 0 / 21 loss: 0.12528841197490692
Epoch 8 -- batch 0 / 21 loss: 0.13290080428123474
Epoch 9 -- batch 0 / 21 loss: 0.14169876277446747
Epoch 10 -- batch 0 / 21 loss: 0.12922433018684387
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 3
ranks: tensor([184., 230., 172., 202., 182.,  13.,  20.,  30., 102., 151., 150., 113.,
        129., 195., 188., 185., 230.,  81., 202., 227., 253., 127.,  91.,  59.,
         51., 164.,  74.,  94., 125., 244., 242., 249., 250.,  22., 226., 134.,
        249., 244., 247., 241., 229., 222., 229., 232., 231.,  34., 243., 241.,
        255.,  75.,  82.,  13., 232.,  15., 233., 243., 236.,  12.,  19.,  17.,
        243., 235., 211., 222., 220., 222., 226., 217., 220., 153., 148., 195.,
        192.,  27.,   3.,   7.,   1., 133., 203., 205.,   3.,  40.,  79., 144.,
        214.,  77.,  30., 207., 134., 130., 241., 263., 259.,  25., 161.,  71.,
        240., 241., 216.,  92., 134., 238., 134., 119., 218., 220., 250., 253.,
         83., 257., 227.,  30.,  74.,  15.,  16.,  17.,  19., 249., 235., 252.,
        115., 239.,  94., 210., 245.,  27.,  22., 159., 117.,  63., 216., 208.,
        195.,  48., 161., 206., 128., 222., 246., 258., 243., 176., 159., 147.,
          1.,   1.,   1.,  28., 196., 212., 210., 200., 207.,  73., 163.,  49.,
          6.,   1.,   1., 132.,  74., 223., 120., 112., 216., 189., 178.,   9.,
         94.,  85., 231., 229.,  59., 132.,  52.,  22.,  79., 190.,  16.,  12.,
          7.,  55.,  16., 105.,  13., 219., 118., 133.,  12.,  49., 268., 213.,
        254., 243., 194., 231.,   1.,   6., 206., 160., 229., 186., 142., 135.,
        132., 224., 124., 230.,  31.,   8., 226., 198., 184., 185.,  93., 181.,
          1., 112.,  11.,  17., 146., 230., 169., 191., 191., 233., 232., 240.,
        237.,  76., 104.,   3.,  22., 249., 211., 241., 165., 142.,   1.,   1.,
          1., 243.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1., 233., 155., 160.,  66., 242., 233., 243., 200.,
         41., 221., 181., 124., 243., 220., 219., 223., 210.,   3., 215., 111.,
         85.,  35.,  52.,   9.,  88.,  48.,  93.,   5.,  94.,  93., 117.,  54.,
        230.,   5.,  30., 135., 207., 239., 239., 254., 235., 225.,  85.,  38.,
         33., 233., 215.,  16., 236., 232., 235., 233., 241., 239., 231., 169.,
         28., 223., 225., 197.,   9.,  36., 147., 253., 157., 252., 203., 135.,
         85.,   7.,  14.,  88., 211., 176., 206., 128., 101., 202., 157.,  69.,
        174., 221., 101., 222., 187., 235., 100., 149.,  54., 168., 139., 234.,
        227., 147., 186., 207., 220., 136., 211., 206., 222., 201., 205., 209.,
        206., 155., 239.,  37.,  33.,  31.,  29.,  29.,  99.,   1.,  44.,   7.,
          8.,  18.,   8.,   1., 240., 245., 138., 115., 230.,   1., 129., 179.,
        213., 117., 258., 199., 131., 120., 255., 127., 188.,  67.,  28.,  27.,
        131., 200., 121., 114., 124.,   6.,   9., 205., 133., 171., 215., 227.,
        198., 210., 210., 154., 220., 215., 245., 235.,  16., 121.,  10., 254.,
        134.,  20., 139., 207., 170., 188.,  88., 125., 256., 256., 196., 246.,
        246., 244., 256.,   1., 256.,   1., 182., 126., 135., 255., 131., 190.,
        161., 197.,  13.,   7., 191.,  20., 190., 195., 223., 223., 153.,   5.,
         22.,   6.,   8.,  14.,  20.,   4., 111., 125., 218., 165., 140., 164.,
         88.,  73.,  42., 177., 104.,  69.,  13.,   1.,   1.,  11., 189., 195.,
        213., 195., 190., 195., 165.,  19., 209.,   3., 206., 207., 129.,  37.,
        203., 120., 207., 154., 130., 219.,  57.,   1.,   1.,   1.,   1.,   1.,
          1., 217., 210., 227.,  15., 216., 216.,  69.,  85., 109.,  79., 139.,
        145., 129., 116.,  18., 125., 156., 182., 185., 193., 212., 162., 176.,
        132., 129.,   1.,   1.,   1., 239., 211., 194., 197., 182., 146., 113.,
        226., 228., 221., 224., 109., 217.,  24.,  10., 112., 237., 200., 241.,
         30., 229., 234., 203., 221., 238., 246., 246., 204., 240., 166.,  84.,
        231., 246., 238.,  37.,  67., 196., 235., 199.,  12.,  19.,  36., 141.,
        152.,  33., 131., 102., 169.,  78., 110.,  93., 190., 113., 171., 181.,
         20.,   7., 173., 205., 182.,   1.,   1., 201., 114., 113., 109.,  98.,
        121.,  86., 116., 227.,   1.,  20., 214.,  41.,  19., 214., 175., 239.,
        254., 152.,  12.,   7.,  89., 135., 249., 253.,  12.,  20., 251., 249.,
          1., 127., 144., 133., 201.,  12.,   4.,   5.,   1.,   1.,   1.,   1.,
         48., 100.,   1.,   1.,  17., 122., 152.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 133.71319580078125
mrr: 0.11193929612636566
h1: 0.0920245423913002
h3: 0.09969325363636017
h5: 0.10889570415019989
h10: 0.1411042958498001
==================================

Done Testing!
done with training and eval
Experiments took 37 seconds
