['run_exp.py', '0', 'UMLS-Kinships', '10', '5e-6', 'zscore', '32', '30', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
Kinships
X_p: torch.Size([8544, 37])
X_p: torch.Size([1074, 37])
X_p: torch.Size([1068, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 430 loss: 0.1776835024356842
Epoch 2 -- batch 0 / 430 loss: 0.11078620702028275
Epoch 3 -- batch 0 / 430 loss: 0.1338568925857544
Epoch 4 -- batch 0 / 430 loss: 0.150064155459404
Epoch 5 -- batch 0 / 430 loss: 0.10074785351753235
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 430 loss: 0.15493597090244293
Epoch 7 -- batch 0 / 430 loss: 0.15681232511997223
Epoch 8 -- batch 0 / 430 loss: 0.16199567914009094
Epoch 9 -- batch 0 / 430 loss: 0.11791326850652695
Epoch 10 -- batch 0 / 430 loss: 0.1268250048160553
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 21
ranks: tensor([193., 230., 185., 207., 173.,   5.,   1.,  22., 143., 101., 120.,  62.,
         82., 185., 190., 186., 221.,  76., 203., 244., 255., 153.,  29.,  39.,
         42., 164., 107., 115., 113.,   1.,   1.,   1., 250., 124., 215., 118.,
        216., 235., 246., 244., 207., 217., 223., 226., 224., 119., 233., 241.,
        255., 222., 127.,  30., 241.,  21., 224., 243., 240.,  17.,  19.,  15.,
        241., 227., 225., 240., 237., 216., 220., 203., 217., 101., 131., 197.,
        115.,  36.,   1., 108.,   3., 151., 203., 205.,  14.,  62.,  78., 117.,
        214.,  48.,  68., 174., 125.,  74., 116.,   1.,   1.,  99., 134.,  44.,
        145., 230., 233., 209., 239., 249., 246., 238., 249., 244., 250., 253.,
         27., 255.,  93.,  98., 123., 110., 160., 141., 156., 255., 251., 257.,
        247., 230.,  62., 188., 236.,  19.,  28.,  19.,  25.,  44., 190., 192.,
         33.,   1.,  83., 130., 113., 205., 242., 257., 252.,  29., 169., 175.,
          1.,   1.,   1.,  25., 201., 207., 212., 206., 207., 162., 178., 169.,
        117., 101.,  90.,  43., 157., 223., 144., 188., 196., 180., 117.,   2.,
        124.,  48., 231., 229., 152., 187., 186.,  70., 195., 186.,  30.,  14.,
         25.,   6.,  89., 109.,   1.,  91., 237., 118.,   1.,  15.,   1.,   1.,
        235., 204., 162., 198.,   1.,   1.,  56., 147., 167., 139., 144., 132.,
         40., 220., 154., 230.,  80.,  43., 230., 198., 181., 172., 167., 177.,
          1.,  79.,  46.,  78., 174., 230., 105.,   1., 185., 217., 217., 230.,
        200., 169., 157.,   8., 103., 232., 130., 217., 225., 197., 220., 216.,
        242., 243., 134., 132.,  67., 188., 235., 216., 234., 246., 232., 255.,
        254., 255.,  70.,   1., 237., 198., 186.,  85., 235., 227., 243., 190.,
        108., 226., 154., 105., 243., 187., 222., 223., 215.,   4., 214., 193.,
        141.,  23.,  94.,   7., 153.,  28.,  87.,  10.,  57.,  75., 101.,  34.,
        236.,  13.,  91., 134., 207., 236., 241., 256., 241., 219.,  33.,  29.,
         28., 239., 217.,  16., 228., 240., 236., 237., 238., 235., 245., 206.,
         40., 222., 202., 203.,  18., 201., 172.,   1.,  28., 252., 110., 135.,
         21.,   9.,  29., 194., 212., 197., 207., 198., 154., 203., 176.,  55.,
        177., 226., 163., 223., 190., 236.,  41., 132., 112., 192., 187., 236.,
        188., 129.,   1.,   1., 186., 194., 239., 237., 238., 122., 187., 212.,
        148., 129., 233., 202., 198., 205., 201.,  93., 182.,  19.,  43.,  19.,
         29.,  19.,  17.,   6., 240.,  64., 115.,  25.,  32.,  38., 135.,   1.,
        259., 223., 260., 251., 246., 153., 255., 207., 249., 248., 206., 199.,
        163., 180.,   1., 132., 132.,  60., 108., 204., 172., 170., 206., 188.,
         30.,  58.,  75.,  87., 140., 123., 130.,   1.,  32.,  75.,   1., 219.,
        217., 195.,  79., 169., 147., 192., 159., 185., 253., 256., 132., 200.,
        236., 241., 253.,  22., 251.,   1.,  84.,  65., 135., 255.,  43., 175.,
        160.,  66.,   1.,   1., 191.,   1., 191., 161., 222., 223., 153.,   1.,
          1.,   1.,   1.,   3.,   6.,   1.,  55.,  52., 124.,  91.,  15.,  91.,
        111., 123., 116., 189., 190.,  19., 107., 107.,  93., 120.,   1.,   1.,
        194., 182., 181., 180., 144., 109., 195.,   1., 185., 183.,  17., 111.,
        180., 109., 204., 168., 139., 183., 171., 212., 243.,  50., 104., 204.,
        174., 213., 129., 215.,   5.,  79., 192.,  40.,  14.,  92.,  88., 202.,
        209., 121., 111.,  13., 103., 133., 163., 195., 169., 210., 156., 164.,
        127., 143.,   1.,   1.,   1., 220., 191., 177., 179., 189., 177., 115.,
        231., 234.,   1.,   1.,  83., 190.,  83.,  56.,  97., 237., 191., 239.,
         41., 203., 206., 193., 186., 131., 246., 246., 108., 243., 207., 149.,
        246., 246., 242., 177.,  93., 201., 240., 208.,  59., 194.,   5.,  96.,
         71.,  82., 111.,  64., 169.,  73.,  72., 177., 202., 178., 193., 167.,
          3.,   1., 131., 190., 188.,   7.,  10., 193., 121.,  72.,  22.,  77.,
        164., 136.,  37., 239.,   1.,   1.,   1.,   1., 109.,  62., 174., 192.,
        254., 135.,   1.,  12.,  17., 135., 248., 253.,   2.,   2., 211., 199.,
         15.,   3., 132.,  71.,  34.,   3.,   1.,   1., 127., 180.,   1.,  31.,
          1.,  32.,   1.,   1.,  85.,  86., 241., 125.,  19., 247., 208., 123.,
        110., 164., 108.,  28.])
mr: 137.0107421875
mrr: 0.10384544730186462
h1: 0.08588957041501999
h3: 0.09815950691699982
h5: 0.10429447889328003
h10: 0.11809816211462021
Testing (cite this): dataloader for dataset Kinships
ranks: tensor([193.0000, 230.0000, 185.0000,  ..., 180.0000, 183.0000,  40.5000])
mr: 133.8874969482422
mrr: 0.13569945096969604
h1: 0.11744186282157898
h3: 0.13081395626068115
h5: 0.13604651391506195
h10: 0.15813954174518585
==================================

Done Testing!
done with training and eval
Experiments took 93 seconds
