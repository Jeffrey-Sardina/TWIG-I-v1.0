['run_exp.py', '0', 'UMLS-Kinships', '10', '5e-6', 'zscore', '32', '100', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
Kinships
X_p: torch.Size([8544, 37])
X_p: torch.Size([1074, 37])
X_p: torch.Size([1068, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 430 loss: 0.1792120337486267
Epoch 2 -- batch 0 / 430 loss: 0.11138048022985458
Epoch 3 -- batch 0 / 430 loss: 0.131430521607399
Epoch 4 -- batch 0 / 430 loss: 0.14665831625461578
Epoch 5 -- batch 0 / 430 loss: 0.10015303641557693
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 430 loss: 0.15994787216186523
Epoch 7 -- batch 0 / 430 loss: 0.15893647074699402
Epoch 8 -- batch 0 / 430 loss: 0.15834927558898926
Epoch 9 -- batch 0 / 430 loss: 0.12484479695558548
Epoch 10 -- batch 0 / 430 loss: 0.1216290071606636
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 21
ranks: tensor([193., 230., 185., 207., 173.,   7.,   1.,  19., 140., 109., 106.,  67.,
         65., 188., 189., 186., 222.,  79., 207., 240., 255., 154.,  23.,  42.,
         36., 165., 112., 121., 116.,   1.,   1.,   1., 250., 124., 215., 118.,
        216., 236., 246., 245., 208., 215., 230., 221., 222., 123., 234., 243.,
        255., 220., 141.,  26., 243.,  22., 229., 243., 241.,  21.,  20.,  15.,
        238., 224., 229., 242., 238., 216., 217., 203., 217., 101., 133., 200.,
        109.,  33.,   1., 107.,   8., 148., 203., 207.,  15.,  73.,  65., 120.,
        213.,  51.,  70., 173., 127.,  85., 119.,   1.,   1.,  96., 132.,  44.,
        145., 230., 233., 209., 239., 249., 247., 239., 249., 244., 250., 253.,
         34., 257.,  90.,  92., 126., 121., 154., 138., 160., 255., 251., 257.,
        247., 226.,  61., 188., 240.,  23.,  27.,  21.,  26.,  44., 194., 192.,
         29.,   1.,  81., 132., 106., 208., 239., 256., 252.,  28., 174., 176.,
          1.,   1.,   1.,  24., 201., 206., 212., 205., 207., 165., 180., 178.,
        109.,  94., 103.,  41., 158., 223., 150., 185., 194., 182., 120.,   2.,
        123.,  47., 231., 229., 143., 206., 175.,  69., 192., 185.,  26.,  18.,
         29.,   4.,  96.,  98.,   1., 100., 237., 114.,   1.,  14.,   1.,   1.,
        234., 203., 164., 200.,   1.,   1.,  56., 155., 165., 141., 135., 127.,
         47., 220., 145., 230.,  70.,  53., 230., 198., 190., 179., 159., 190.,
          1.,  76.,  49.,  75., 177., 230.,  86.,   1., 185., 218., 218., 231.,
        200., 176., 156.,   7., 109., 232., 127., 215., 224., 195., 223., 214.,
        244., 243., 137., 136.,  71., 183., 239., 218., 233., 246., 234., 255.,
        253., 255.,  70.,   1., 237., 197., 187.,  89., 235., 226., 243., 194.,
        110., 225., 162., 110., 243., 191., 222., 223., 215.,   9., 215., 195.,
        141.,  24.,  98.,   7., 159.,  31.,  86.,   6.,  58.,  79.,  92.,  37.,
        236.,  15.,  93., 134., 210., 232., 244., 256., 237., 222.,  40.,  22.,
         35., 239., 214.,  21., 227., 237., 238., 234., 240., 234., 248., 205.,
         39., 227., 203., 199.,  18., 201., 174.,   1.,  28., 250., 105., 135.,
         19.,  12.,  26., 195., 212., 194., 207., 200., 163., 198., 173.,  68.,
        175., 228., 166., 223., 190., 236.,  37., 133., 110., 191., 198., 236.,
        186., 130.,   1.,   1., 186., 195., 237., 238., 238., 123., 188., 215.,
        143., 131., 229., 205., 199., 204., 201.,  95., 183.,  22.,  48.,  18.,
         32.,  19.,  24.,   5., 239.,  60., 116.,  14.,  32.,  35., 135.,   1.,
        259., 222., 260., 250., 249., 148., 255., 210., 248., 247., 208., 200.,
        163., 177.,   1., 132., 132.,  70.,  97., 203., 175., 176., 182., 193.,
         29.,  63.,  74.,  85., 140., 123., 132.,   1.,  32.,  79.,   1., 224.,
        219., 194.,  77., 169., 152., 194., 149., 189., 251., 256., 130., 207.,
        231., 242., 254.,  15., 250.,   1.,  81.,  58., 135., 255.,  45., 189.,
        155.,  61.,   1.,   1., 191.,   1., 192., 161., 221., 223., 149.,   1.,
          1.,   1.,   1.,   2.,   7.,   1.,  64.,  45., 124.,  93.,  13., 106.,
        104., 119., 106., 189., 192.,  19.,  98., 109.,  96., 116.,   1.,   1.,
        191., 183., 181., 180., 142., 110., 195.,   1., 182., 182.,  17., 109.,
        181., 107., 203., 171., 132., 181., 161., 210., 243.,  58., 105., 205.,
        172., 210., 132., 214.,   6.,  77., 193.,  37.,  11.,  88.,  86., 207.,
        205., 120., 110.,  12., 101., 138., 160., 197., 175., 207., 157., 165.,
        115., 159.,   1.,   1.,   1., 218., 188., 181., 178., 189., 178., 111.,
        231., 234.,   1.,   1.,  81., 189.,  84.,  52.,  97., 237., 191., 239.,
         42., 199., 210., 184., 188., 120., 246., 246., 101., 245., 203., 152.,
        246., 246., 241., 182.,  89., 208., 240., 208.,  58., 189.,   7.,  88.,
         73.,  91., 110.,  65., 169.,  79.,  63., 175., 202., 176., 192., 163.,
          1.,   1., 132., 191., 184.,   2.,  16., 196., 109.,  87.,  23.,  75.,
        163., 136.,  37., 238.,   1.,   1.,   1.,   1., 114.,  66., 174., 194.,
        254., 133.,   1.,  10.,  17., 135., 247., 253.,   2.,   1., 203., 205.,
         18.,   5., 133.,  71.,  36.,   3.,   1.,   1., 138., 181.,   1.,  28.,
          1.,  38.,   1.,   1.,  85.,  86., 241., 125.,  18., 247., 199., 116.,
        113., 169., 109.,  27.])
mr: 137.1104278564453
mrr: 0.1056726723909378
h1: 0.0889570564031601
h3: 0.09662576764822006
h5: 0.10122699290513992
h10: 0.11656441539525986
Testing (cite this): dataloader for dataset Kinships
ranks: tensor([193., 230., 185.,  ..., 180., 183.,  41.])
mr: 133.73458862304688
mrr: 0.13635003566741943
h1: 0.11860465258359909
h3: 0.1302325576543808
h5: 0.1366279125213623
h10: 0.1563953459262848
==================================

Done Testing!
done with training and eval
Experiments took 117 seconds
