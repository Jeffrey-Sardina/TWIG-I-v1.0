['run_exp.py', '0', 'UMLS-Kinships', '10', '5e-6', 'zscore', '256', '100', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
Kinships
X_p: torch.Size([8544, 37])
X_p: torch.Size([1074, 37])
X_p: torch.Size([1068, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 55 loss: 0.13447906076908112
Epoch 2 -- batch 0 / 55 loss: 0.13715659081935883
Epoch 3 -- batch 0 / 55 loss: 0.13750778138637543
Epoch 4 -- batch 0 / 55 loss: 0.14362062513828278
Epoch 5 -- batch 0 / 55 loss: 0.1372918337583542
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 55 loss: 0.13564904034137726
Epoch 7 -- batch 0 / 55 loss: 0.14510072767734528
Epoch 8 -- batch 0 / 55 loss: 0.14016534388065338
Epoch 9 -- batch 0 / 55 loss: 0.13162511587142944
Epoch 10 -- batch 0 / 55 loss: 0.14091719686985016
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 3
ranks: tensor([184., 230., 175., 202., 188.,  28.,  34.,  49., 142., 175., 166., 137.,
        125., 196., 191., 190., 230.,  96., 212., 233., 254., 131.,  77.,  93.,
         43., 163.,  90., 114., 128., 239., 239., 249., 250.,  17., 224., 135.,
        250., 242., 248., 242., 225., 221., 229., 228., 221.,  35., 241., 241.,
        255.,  87.,  64.,  20., 234.,  19., 234., 241., 238.,  17.,  16.,  15.,
        243., 228., 212., 223., 168., 222., 224., 216., 220., 155., 137., 161.,
        161.,  21.,   3.,  31.,   1., 127., 203., 190.,   4.,  36.,  86., 155.,
        214.,  79.,  23., 204., 135., 127., 242., 264., 259.,  26., 161.,  62.,
        242., 238., 219.,  86., 139., 235., 127., 107., 224., 197., 250., 253.,
         70., 257., 225.,  31.,  95.,  17.,  18.,  17.,  28., 245., 234., 248.,
         66., 239.,  69., 191., 244.,  26.,  29., 141.,  94.,  50., 211., 200.,
        192.,  39., 158., 202., 127., 226., 242., 258., 237., 170., 146., 128.,
          1.,   1.,   1.,  22., 187., 203., 209., 199., 207., 150., 177., 108.,
          8.,   1.,   8., 153., 127., 223., 119., 169., 199., 176., 166.,   8.,
         94.,  86., 229., 229.,  58., 124.,  60.,  26.,  80., 174.,  19.,  11.,
          8.,  57.,  17., 103.,  13., 221., 154., 159.,  13.,  33., 268., 205.,
        254., 236., 189., 231.,   1.,  16., 213., 163., 230., 189., 140., 135.,
        128., 225., 170., 230.,  57.,  28., 230., 198., 183., 195.,  75., 181.,
          1., 129.,  16.,  17., 135., 230., 160., 211., 194., 226., 229., 241.,
        229.,  76., 130.,  14.,  15., 247., 196., 238., 164., 138.,   1.,   1.,
          1., 243.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1., 234., 161., 151.,  79., 242., 231., 243., 204.,
         53., 219., 170., 109., 243., 218., 215., 223., 215.,   4., 217., 128.,
        115.,  31.,  92.,  11., 103.,  52.,  96.,   5.,  81.,  96., 114.,  48.,
        231.,   3.,  30., 135., 214., 239., 241., 256., 239., 218.,  58.,  39.,
         32., 234., 213.,  20., 236., 232., 230., 231., 242., 240., 234., 200.,
         30., 229., 221., 190.,   8.,  59., 136., 253., 156., 252., 202., 135.,
         88.,   7.,  16., 125., 210., 163., 205., 116., 146., 210., 169.,  59.,
        182., 222., 115., 221., 186., 234.,  88., 147.,  60., 169., 160., 232.,
        220., 169., 194., 206., 206., 128., 214., 197., 217., 203., 206., 205.,
        202., 147., 239.,  44.,  40.,  33.,  30.,  32., 129.,   6.,  35.,   7.,
          5.,  28.,   5.,   1., 239., 242., 133., 119., 225.,   4., 130., 155.,
        200., 114., 258., 194., 144., 119., 255., 122., 191.,  85.,  23.,  44.,
        137., 186., 118., 116., 114.,   7.,   8., 199., 174., 173., 212., 221.,
        194., 208., 210., 174., 220., 213., 245., 226.,  16., 106.,   8., 249.,
        136.,  21., 128., 214., 162., 207., 123., 129., 256., 256., 201., 246.,
        246., 243., 256.,   1., 256.,   2., 171., 119., 135., 255., 123., 174.,
        133., 179.,  13.,   1., 191.,  21., 190., 184., 223., 223., 143.,   1.,
         25.,   6.,   9.,  18.,  42.,   4., 133., 134., 216., 168., 143., 156.,
         84.,  78.,  39., 177.,  88.,  54.,  10.,   1.,   1.,  15., 193., 194.,
        215., 183., 174., 183., 171.,  11., 210.,   1., 210., 199., 123.,  39.,
        206., 124., 206., 137., 125., 214.,  52.,   1.,   1.,   1.,   1.,   1.,
          1., 213., 202., 224.,  13., 217., 213.,  66.,  65.,  96.,  86., 160.,
        176., 133., 119.,  19., 126., 160., 187., 202., 180., 212., 158., 171.,
        116., 158.,   1.,   1.,   1., 235., 206., 184., 187., 190., 169., 113.,
        226., 228., 206., 216., 101., 216.,  32.,   5., 107., 237., 187., 236.,
         23., 229., 234., 194., 223., 230., 246., 246., 195., 240., 153.,  85.,
        232., 246., 238.,  46.,  96., 205., 234., 207.,  21.,  31.,  31., 134.,
        160.,  38., 131., 108., 169., 189., 106., 101., 189.,  84., 160., 184.,
         20.,   5., 179., 200., 182.,   6.,   2., 202., 131., 105., 108.,  93.,
        120.,  85., 116., 226.,   1.,  18., 218.,  34.,  22., 211., 187., 240.,
        254., 150.,  15.,   7.,  72., 135., 249., 253.,   6.,  10., 251., 248.,
          1., 140., 144., 136., 197.,  18.,   5.,   5.,   1.,   1.,   1.,   1.,
         45., 114.,   1.,   1.,  81., 123., 152.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 134.44325256347656
mrr: 0.10789883136749268
h1: 0.08742330968379974
h3: 0.09355828166007996
h5: 0.11042945086956024
h10: 0.1380368024110794
Testing (cite this): dataloader for dataset Kinships
ranks: tensor([184., 230., 175.,  ..., 180., 183.,  93.])
mr: 137.98545837402344
mrr: 0.10442649573087692
h1: 0.0872092992067337
h3: 0.09302325546741486
h5: 0.1040697693824768
h10: 0.12790697813034058
==================================

Done Testing!
done with training and eval
Experiments took 116 seconds
