['run_exp.py', '0', 'UMLS-Kinships', '10', '5e-6', 'zscore', '256', '30', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
Kinships
X_p: torch.Size([8544, 37])
X_p: torch.Size([1074, 37])
X_p: torch.Size([1068, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 55 loss: 0.13475216925144196
Epoch 2 -- batch 0 / 55 loss: 0.13893437385559082
Epoch 3 -- batch 0 / 55 loss: 0.13517460227012634
Epoch 4 -- batch 0 / 55 loss: 0.14470620453357697
Epoch 5 -- batch 0 / 55 loss: 0.13679127395153046
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 55 loss: 0.1355895698070526
Epoch 7 -- batch 0 / 55 loss: 0.14520254731178284
Epoch 8 -- batch 0 / 55 loss: 0.13998804986476898
Epoch 9 -- batch 0 / 55 loss: 0.13158433139324188
Epoch 10 -- batch 0 / 55 loss: 0.14386233687400818
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 3
ranks: tensor([184., 230., 175., 202., 188.,  24.,  29.,  58., 146., 167., 177., 130.,
        142., 194., 191., 186., 230.,  96., 205., 236., 254., 130.,  78.,  95.,
         53., 160.,  84., 112., 122., 241., 242., 249., 250.,  17., 223., 140.,
        250., 242., 245., 242., 224., 221., 227., 229., 221.,  37., 241., 241.,
        255.,  81.,  64.,  18., 234.,  19., 230., 242., 238.,  13.,  15.,  18.,
        243., 236., 206., 224., 169., 222., 225., 215., 220., 150., 144., 160.,
        174.,  26.,   2.,  29.,   1., 136., 203., 191.,   3.,  38.,  85., 149.,
        218.,  86.,  26., 204., 133., 131., 239., 263., 260.,  23., 166.,  64.,
        239., 239., 215.,  87., 141., 236., 129., 110., 223., 198., 250., 253.,
         74., 257., 228.,  32.,  94.,  17.,  16.,  19.,  28., 248., 232., 251.,
         69., 238.,  67., 195., 245.,  27.,  25., 142.,  92.,  53., 210., 200.,
        193.,  33., 156., 199., 125., 218., 246., 258., 233., 172., 146., 128.,
          1.,   1.,   1.,  25., 192., 208., 209., 201., 207., 149., 179., 103.,
          9.,   1.,   8., 148., 131., 223., 120., 164., 198., 181., 168.,  10.,
        103.,  84., 230., 229.,  60., 127.,  64.,  28.,  69., 183.,  16.,  13.,
          9.,  66.,  15., 101.,  13., 219., 157., 163.,  12.,  41., 268., 212.,
        254., 234., 188., 221.,   1.,  14., 220., 164., 233., 190., 133., 135.,
        132., 224., 166., 230.,  57.,  36., 230., 198., 191., 180.,  75., 193.,
          1., 127.,  17.,  14., 139., 230., 157., 208., 194., 229., 230., 239.,
        233.,  76., 127.,  15.,  15., 247., 196., 246., 165., 139.,   1.,   1.,
          1., 243.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1., 234., 162., 152.,  74., 242., 232., 243., 202.,
         54., 220., 166., 104., 243., 217., 218., 223., 214.,   1., 214., 133.,
        115.,  30.,  89.,  12.,  93.,  42.,  95.,   7.,  79.,  87., 123.,  44.,
        234.,   3.,  25., 135., 217., 240., 239., 256., 237., 219.,  65.,  36.,
         31., 233., 211.,  21., 237., 233., 229., 229., 241., 240., 231., 203.,
         31., 225., 221., 194.,   7.,  58., 137., 253., 157., 252., 201., 135.,
         77.,   6.,  14., 121., 210., 163., 204., 120., 143., 212., 165.,  70.,
        181., 224., 123., 221., 187., 234.,  88., 144.,  57., 179., 155., 233.,
        218., 157., 201., 197., 210., 127., 219., 191., 217., 204., 203., 207.,
        203., 142., 239.,  44.,  40.,  32.,  31.,  33., 127.,   5.,  48.,   7.,
          7.,  23.,   7.,   1., 238., 241., 131., 109., 230.,   5., 132., 153.,
        202., 114., 259., 200., 142., 120., 255., 127., 198.,  81.,  33.,  39.,
        136., 192., 113., 118., 115.,   7.,  11., 199., 170., 172., 213., 223.,
        188., 203., 208., 176., 220., 211., 245., 224.,  18., 110.,   6., 248.,
        133.,  19., 134., 207., 165., 197., 130., 130., 256., 256., 197., 245.,
        248., 242., 256.,   1., 256.,   2., 167., 122., 135., 255., 131., 173.,
        131., 182.,   7.,   1., 191.,  21., 192., 180., 223., 223., 145.,   1.,
         20.,   6.,  10.,  15.,  41.,   7., 129., 132., 216., 170., 140., 155.,
         90.,  73.,  37., 174.,  93.,  51.,  13.,   1.,   1.,  13., 193., 198.,
        214., 184., 172., 185., 174.,  16., 206.,   1., 207., 203., 122.,  42.,
        198., 125., 206., 134., 126., 219.,  56.,   1.,   1.,   1.,   1.,   1.,
          1., 215., 203., 224.,  22., 216., 214.,  65.,  65.,  99.,  89., 156.,
        177., 134., 119.,  19., 126., 158., 186., 198., 174., 212., 157., 171.,
        126., 145.,   1.,   1.,   1., 237., 207., 179., 191., 190., 169., 115.,
        225., 232., 213., 223.,  96., 207.,  31.,   3., 110., 237., 188., 235.,
         21., 222., 235., 200., 224., 233., 246., 246., 186., 241., 170.,  73.,
        226., 246., 236.,  48.,  96., 199., 233., 209.,  21.,  32.,  42., 135.,
        159.,  43., 132., 109., 169., 171., 110., 100., 190.,  76., 156., 181.,
         19.,   4., 174., 201., 182.,   4.,   2., 200., 124., 113., 115.,  92.,
        124.,  95., 121., 228.,   1.,  16., 214.,  43.,  20., 208., 184., 239.,
        254., 151.,  15.,   6.,  81., 135., 247., 253.,   8.,   9., 251., 249.,
          1., 134., 144., 136., 200.,  17.,   8.,   2.,   1.,   1.,   1.,   1.,
         36., 114.,   1.,   1.,  89., 123., 152.,   1.,   1.,   1.,   1.,   1.,
          1.,   1.,   1.,   1.])
mr: 134.51380920410156
mrr: 0.10975933820009232
h1: 0.0889570564031601
h3: 0.09969325363636017
h5: 0.10582821816205978
h10: 0.13650307059288025
Testing (cite this): dataloader for dataset Kinships
ranks: tensor([184., 230., 175.,  ..., 180., 183.,  92.])
mr: 137.95843505859375
mrr: 0.10541719198226929
h1: 0.08779069781303406
h3: 0.09476744383573532
h5: 0.1029069796204567
h10: 0.12732557952404022
==================================

Done Testing!
done with training and eval
Experiments took 88 seconds
