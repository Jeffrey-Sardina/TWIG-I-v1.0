['run_exp.py', '0', 'UMLS-Kinships', '10', '5e-6', 'zscore', '32', '5', '0', '1', 'simple', '1']
loading NN
done loading NN
loading dataset
UMLS
X_p: torch.Size([5216, 37])
X_p: torch.Size([661, 37])
X_p: torch.Size([652, 37])
Kinships
X_p: torch.Size([8544, 37])
X_p: torch.Size([1074, 37])
X_p: torch.Size([1068, 37])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
loading precalculated corruptions from cache
loading triple features from cache
loading precalculated corruptions from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 430 loss: 0.15859799087047577
Epoch 2 -- batch 0 / 430 loss: 0.11333048343658447
Epoch 3 -- batch 0 / 430 loss: 0.12759290635585785
Epoch 4 -- batch 0 / 430 loss: 0.1539236307144165
Epoch 5 -- batch 0 / 430 loss: 0.092646025121212
NOT Saving checkpoint at epoch 5
Epoch 6 -- batch 0 / 430 loss: 0.16954706609249115
Epoch 7 -- batch 0 / 430 loss: 0.15664370357990265
Epoch 8 -- batch 0 / 430 loss: 0.1641828566789627
Epoch 9 -- batch 0 / 430 loss: 0.12399593740701675
Epoch 10 -- batch 0 / 430 loss: 0.12895536422729492
NOT Saving checkpoint at epoch 10
Done Training!

==================================
Testing (cite this): dataloader for dataset UMLS
Testing (cite this): batch 0 / 21
ranks: tensor([193., 230., 184., 206., 178.,   3.,   2.,  23., 151., 105., 134.,  78.,
         74., 191., 188., 187., 226.,  86., 205., 246., 255., 151.,  27.,  42.,
         41., 164., 101., 117., 113.,   1.,   1.,   1., 250., 124., 215., 118.,
        221., 235., 245., 243., 206., 219., 225., 228., 221., 126., 235., 240.,
        255., 201., 136.,  20., 242.,  21., 229., 243., 237.,  22.,  15.,  19.,
        239., 218., 230., 240., 237., 216., 222., 197., 216., 102., 133., 202.,
        108.,  34.,   1.,  84.,   5., 155., 203., 211.,  10.,  64.,  71., 126.,
        215.,  46.,  59., 170., 124.,  77., 112.,   1.,   1.,  96., 135.,  41.,
        147., 229., 232., 208., 239., 249., 245., 238., 248., 243., 250., 253.,
         29., 256.,  87.,  99., 123., 111., 156., 145., 148., 255., 248., 257.,
        247., 223.,  62., 194., 239.,  28.,  26.,  20.,  21.,  44., 190., 188.,
         25.,   2.,  69., 136., 114., 207., 242., 257., 250.,  28., 175., 169.,
          1.,   1.,   1.,  28., 199., 207., 212., 206., 207., 166., 181., 172.,
         99.,  81.,  98.,  47., 157., 223., 135., 189., 193., 182., 118.,   2.,
        127.,  47., 231., 229., 140., 191., 186.,  64., 200., 188.,  27.,  21.,
         31.,   9.,  90., 108.,   1., 104., 243., 120.,   1.,  16.,   1.,   1.,
        235., 205., 155., 200.,   1.,   1.,  50., 149., 168., 146., 137., 128.,
         37., 225., 154., 230.,  70.,  53., 230., 198., 183., 184., 157., 181.,
          1.,  78.,  55.,  87., 182., 230.,  82.,   1., 185., 212., 220., 230.,
        200., 171., 161.,   7., 100., 235., 128., 213., 226., 193., 223., 216.,
        242., 243., 142., 132.,  73., 182., 237., 221., 230., 246., 232., 255.,
        253., 255.,  70.,   1., 237., 189., 181.,  88., 237., 229., 243., 188.,
        108., 227., 159., 105., 243., 188., 221., 223., 214.,   4., 218., 185.,
        151.,  26.,  96.,   6., 156.,  30.,  87.,   8.,  60.,  72.,  96.,  40.,
        236.,  13.,  91., 135., 209., 231., 240., 256., 238., 218.,  43.,  28.,
         27., 240., 221.,  16., 231., 236., 237., 235., 237., 230., 246., 208.,
         45., 227., 206., 202.,  21., 207., 170.,   1.,  28., 249., 119., 135.,
         15.,  11.,  31., 193., 212., 193., 207., 199., 156., 201., 185.,  61.,
        177., 226., 153., 223., 190., 236.,  43., 117., 120., 192., 191., 236.,
        190., 136.,   1.,   1., 186., 190., 238., 242., 243., 120., 191., 203.,
        147., 131., 233., 203., 197., 203., 203.,  97., 176.,  11.,  45.,  19.,
         33.,  23.,  20.,   7., 238.,  50., 118.,  18.,  27.,  36., 135.,   1.,
        259., 221., 260., 250., 249., 165., 255., 205., 249., 248., 205., 198.,
        162., 166.,   1., 132., 133.,  63., 108., 196., 176., 171., 194., 200.,
         30.,  65.,  72.,  85., 141., 134., 133.,   1.,  28.,  74.,   1., 228.,
        221., 191.,  85., 179., 150., 194., 141., 184., 253., 256., 114., 212.,
        236., 241., 253.,  19., 252.,   1.,  79.,  66., 134., 255.,  42., 177.,
        166.,  58.,   1.,   1., 191.,   1., 193., 156., 223., 223., 147.,   1.,
          1.,   1.,   1.,   2.,   7.,   1.,  62.,  46., 122.,  95.,  20., 103.,
        106., 129., 108., 196., 191.,  16., 111., 107.,  90., 117.,   1.,   1.,
        194., 183., 182., 181., 144., 112., 195.,   1., 188., 184.,  16., 110.,
        182., 109., 199., 168., 140., 189., 190., 215., 243.,  42.,  97., 201.,
        161., 208., 135., 216.,   4.,  82., 199.,  41.,  17.,  88.,  84., 201.,
        208., 119., 109.,  15., 104., 146., 160., 189., 184., 208., 155., 164.,
        127., 157.,   1.,   1.,   1., 219., 192., 179., 179., 189., 175., 104.,
        230., 233.,   1.,   1.,  83., 191.,  83.,  55.,  97., 237., 190., 238.,
         42., 207., 212., 191., 187., 139., 246., 246., 104., 242., 206., 144.,
        245., 246., 242., 168.,  89., 207., 239., 211.,  58., 192.,   4.,  91.,
         67.,  84., 115.,  63., 169.,  83.,  67., 183., 200., 179., 191., 167.,
          2.,   1., 145., 192., 186.,   5.,   7., 199., 122.,  77.,  23.,  79.,
        163., 145.,  41., 238.,   1.,   1.,   1.,   1.,  85.,  55., 169., 194.,
        254., 139.,   1.,  13.,  21., 135., 248., 253.,   1.,   1., 203., 202.,
         12.,   1., 137.,  86.,  31.,   3.,   1.,   1., 125., 176.,   1.,  28.,
          1.,  29.,   1.,   1.,  85.,  86., 241., 129.,  17., 247., 204., 124.,
        114., 168., 110.,  28.])
mr: 137.10276794433594
mrr: 0.10564036667346954
h1: 0.08742330968379974
h3: 0.09815950691699982
h5: 0.10582821816205978
h10: 0.11809816211462021
Testing (cite this): dataloader for dataset Kinships
ranks: tensor([193., 230., 184.,  ..., 180., 183.,  48.])
mr: 134.60145568847656
mrr: 0.1345396190881729
h1: 0.11744186282157898
h3: 0.12790697813034058
h5: 0.13604651391506195
h10: 0.1523255854845047
==================================

Done Testing!
done with training and eval
Experiments took 99 seconds
